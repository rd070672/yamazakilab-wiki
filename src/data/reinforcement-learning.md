# 強化学習（Reinforcement Learning, RL）

強化学習は、環境との相互作用を通じて「累積報酬」を最大化する行動（意思決定則）を学ぶ枠組みである。材料科学では、組成・プロセス・合成条件・実験順序などの逐次最適化を、閉ループ（自律探索）として定式化するための基盤である。

## 参考ドキュメント
- Sutton, R. S., Barto, A. G., Reinforcement Learning: An Introduction (2nd ed., online)  
  https://incompleteideas.net/book/the-book-2nd.html
- Karpovich, C. et al., Deep reinforcement learning for inverse inorganic materials design, npj Computational Materials (2024)  
  https://www.nature.com/articles/s41524-024-01474-5
- NIMS プレスリリース：自律自動実験のための汎用ソフトウェア NIMS-OS を開発（2023.07.20）  
  https://www.nims.go.jp/press/2023/07/202307200.html


## 1. 位置づけ
材料開発は、1回の実験・計算が高価で、試行錯誤が逐次的であり、測定ノイズや制約が強いという特徴をもつ。このときRLは、次に何を試すかを「長期的な見返り」で評価し、探索と活用のバランスを学べる点が本質である。

典型的な適用先は以下である。
- 組成探索：次に作る・計算する組成を逐次決定し、性能を最大化する
- プロセス最適化：温度、圧力、雰囲気、アニール時間、成膜条件などを逐次調整する
- 合成計画：多段階のレシピ（順序依存の意思決定）を最適化する
- 自律実験・自律計算：ロボット実験＋解析＋次条件提案を閉ループ化する（Self-driving lab）

## 2. 基本の定式化：MDP（マルコフ決定過程）
強化学習は、通常、MDPで定義する。

- 状態（state）：$s \in \mathcal{S}$
- 行動（action）：$a \in \mathcal{A}$
- 遷移（transition）：$P(s' \mid s,a)$
- 報酬（reward）：$r = R(s,a,s')$
- 割引率：$\gamma \in [0,1)$

目的は、方策（policy）$\pi(a\mid s)$ を選び、期待割引和報酬を最大化することである。
$$
J(\pi) = \mathbb{E}_{\pi}\left[\sum_{t=0}^{\infty}\gamma^t r_t \right]
$$

材料探索への対応づけの例である。
- $s$：これまでの探索履歴、候補材料の特徴量、測定条件、装置状態、推定モデルの不確かさ
- $a$：次に試す組成、次に測る条件、合成レシピの次ステップ
- $r$：目的物性の増加、コスト低減、合成成功、制約違反ペナルティなど
- $s\to s'$：実験・計算で得た新データにより探索状態が更新されること

補足として、観測が不完全（装置ドリフト、未観測変数が支配的）な場合はPOMDP（部分観測MDP）として扱うのが自然である。

## 3. 価値関数とベルマン方程式
価値関数は「先を見越した良さ」の定量化である。

状態価値：
$$
V^\pi(s)=\mathbb{E}_{\pi}\left[\sum_{t=0}^{\infty}\gamma^t r_t \mid s_0=s\right]
$$

行動価値：
$$
Q^\pi(s,a)=\mathbb{E}_{\pi}\left[\sum_{t=0}^{\infty}\gamma^t r_t \mid s_0=s, a_0=a\right]
$$

最適行動価値 $Q^\*(s,a)$ はベルマン最適方程式を満たす。
$$
Q^\*(s,a)=\mathbb{E}\left[r+\gamma\max_{a'}Q^\*(s',a')\right]
$$

材料探索では、$Q$ を「この条件を次に試す価値」として解釈できる。

## 4. 代表的アルゴリズム
### 4.1 バンディット（1手先の探索）
もし「次に何を試すか」だけで終わり、状態遷移の長期効果が弱いなら、文脈付きバンディット（contextual bandit）が実用的である。
- $s$：特徴量（文脈）
- $a$：候補条件
- 報酬：その場の性能
材料のハイスループット探索では、実務的にまずここから入ることが多い。

### 4.2 価値ベース（Q-learning系）
行動価値を学習し、$\arg\max_a Q(s,a)$ で行動する系である。基本更新式は
$$
Q(s_t,a_t)\leftarrow Q(s_t,a_t)+\alpha\left[r_t+\gamma\max_{a}Q(s_{t+1},a)-Q(s_t,a_t)\right]
$$
離散行動（候補組成が有限、条件をグリッド化）で扱いやすい。

### 4.3 方策勾配・Actor-Critic（連続制御に強い）
温度・時間・流量など連続変数が支配的なときに有力である。
方策勾配の基礎形は
$$
\nabla_\theta J(\pi_\theta)=\mathbb{E}\left[\nabla_\theta \log \pi_\theta(a\mid s)\, \hat{A}(s,a)\right]
$$
ここで $\hat{A}$ はアドバンテージ推定である。実験条件の微調整、プロセス制御に適する。

### 4.4 モデルベースRL（高価な実験・計算向け）
環境モデル（サロゲート）$\hat{P}, \hat{R}$ を学び、モデル内で計画（planning）してから実験に反映する考え方である。材料では、DFTやCALPHAD、プロセスシミュレータ、あるいは近似モデルを「環境」として併用し、実験回数を節約する設計が重要である。

## 5. 問題設定：何を状態・行動・報酬にするか
RLの成否は、モデルよりも定式化で決まることが多い。

### 5.1 状態設計（s）
- 候補材料の表現：組成特徴量、結晶構造表現、スペクトル埋め込みなど
- 実験履歴：既測定点、失敗条件、測定ノイズ推定
- 予測器の不確かさ：次の探索における探索度合いを制御するための情報

### 5.2 行動設計（a）
- 離散：候補組成リストから選ぶ、プロセスレシピを離散化して選ぶ
- 連続：温度・時間などを連続制御（安全制約とセットで扱う）

### 5.3 報酬設計（r）
単純な物性最大化だけでなく、実務では多目的である。
- 物性：$r_{\mathrm{prop}}$
- コスト：$r_{\mathrm{cost}}$（測定時間、希少元素、温度上限など）
- 失敗ペナルティ：$r_{\mathrm{fail}}$

重み付き和の一例：
$$
r = w_1 r_{\mathrm{prop}} - w_2 r_{\mathrm{cost}} - w_3 \mathrm{Penalty}
$$

制約つき最適化（安全・実現可能性）を明示するなら、例えば
$$
\max_{\pi}\ \mathbb{E}\left[\sum_{t}\gamma^t r_t\right]
\quad \text{s.t.}\quad
\mathbb{E}\left[\sum_{t}\gamma^t c_t\right]\le C
$$
の形でコスト $c_t$（危険、装置制限超過、化学的禁則など）を導入する。

## 6. 逆設計（Inverse design）とRL：生成・探索のつなぎ方
材料の逆設計は、目標物性と実現可能性（化学妥当性、合成可能性、コスト）を同時に満たす探索である。近年は、多目的RLで化学的ガイドライン（形成エネルギーの負、電荷中性、電気陰性度バランスなど）を満たしつつ、多様な無機組成を生成する枠組みが提案されている。

このときRLは、生成モデルや候補生成器を前段に置き、RL側で「どの候補を次に評価するか」「どの方向へ探索を進めるか」を調整する役割を担うことが多い。

## 7. 実験・計算の自律化（閉ループ）での実装指針
自律探索を実装する典型ループは以下である。
1) 実験・計算でデータ取得（ロボット、測定、シミュレーション）
2) 前処理・品質管理（外れ値、校正、単位・条件の統一）
3) 学習・更新（方策、価値関数、またはサロゲート）
4) 次候補選定（安全制約・装置制約も同時にチェック）
5) 実行キューに投入して反復

材料分野では、探索アルゴリズム（RLを含む）とロボット実験をつなぐ実行基盤が鍵であり、汎用オーケストレーション（自律実験の連携基盤）が整備されつつある。

## 8. 評価の観点
- 外挿：未知元素系、未知組成域、未知プロセス域で性能が保たれるか
- データリーク：同一試料の重複、近縁構造の混入、同一バッチの偏りを避ける必要がある
- 実験ノイズ：報酬のばらつきが大きいほど、安定な学習設計（バッチ化、ロバスト化）が重要である
- 実行可能性：提案が化学的・装置的に実行可能かを常に監視する必要がある

## まとめ
強化学習は、材料探索・プロセス最適化・合成計画を、逐次意思決定として統一的に扱うための枠組みである。材料では試行回数が限られるため、問題の定式化（状態・行動・報酬）と、安全制約、モデルベース化や自律実験基盤との統合が実用上の鍵である。
