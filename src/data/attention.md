# アテンション機構と解釈性

Attention機構は、入力の各要素（トークン）が「他のどの要素をどれだけ参照するか」を学習し、重み付き和として情報を集約する仕組みである。材料科学では、元素間・原子環境間・ピーク間・文献中の概念間など、多対多の相互関係を同一の数理で扱える点が重要である。

## 参考ドキュメント
- Vaswani et al., Attention Is All You Need (NeurIPS 2017)
  https://papers.neurips.cc/paper_files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf
- Harvard NLP: The Annotated Transformer（概念と実装の解説）
  https://nlp.seas.harvard.edu/annotated-transformer/
- Qiita：TransformerのSelf-Attentionをざっくり解説（日本語）
  https://qiita.com/age884/items/4ae64961e33d718b6fbd


## 1. 基本概念：Attentionは「学習される重み付き平均」である
トークン集合（または系列）を $X=\{x_1,\dots,x_L\}$ とする。Attentionは本質的に、あるクエリ $q$ に対して、候補のキー $k_i$ と値 $v_i$ の集合から

- 重要度（重み） $\alpha_i$ を計算し
- 出力 $y=\sum_i \alpha_i v_i$ を得る

という「選択的な情報合成」を微分可能な形で実現する機構である。材料の文脈では、$i$ は「元素」「原子」「スペクトルの区間」「文中の語」「測定条件」などに対応させられる。

## 2. Scaled Dot-Product Attention（Transformerで標準の形）
長さ $L$、埋め込み次元 $d$ の入力 $X\in \mathbb{R}^{L\times d}$ から、線形写像で

$$
Q=XW^Q,\quad K=XW^K,\quad V=XW^V
$$

を作る（$Q,K,V\in \mathbb{R}^{L\times d_k}$ など）。このとき注意は

$$
\mathrm{Attn}(Q,K,V)=\mathrm{softmax}\!\left(\frac{QK^\top}{\sqrt{d_k}}\right)V
$$

で定義される。

ここで
- $QK^\top$ は「トークン間の類似度行列」である
- $\mathrm{softmax}$ により行ごとに確率分布（重み）へ正規化される
- $1/\sqrt{d_k}$ は内積のスケールを抑え、学習を安定化するための係数である

材料データに写像すると、$QK^\top$ は例えば「元素特徴の相性」「原子環境の相関」「ピーク同士の同時出現パターン」「文脈上の共起」などを表す“学習された相互作用行列”とみなせる。

## 3. Self-AttentionとCross-Attention
### 3.1 Self-Attention
Self-Attentionでは $Q,K,V$ を同じ入力 $X$ から作るため、各トークンが入力全体を参照して自己表現を更新する機構となる。材料科学では以下の対応が典型である。

- 組成トークン：元素同士の相互依存（例：FeとCoの同時添加の効果）
- 原子トークン：局所環境と中距離秩序の同時モデリング（ただし不変性設計が要点である）
- スペクトルトークン：主ピークだけでなく微小ピークや肩構造への参照

### 3.2 Cross-Attention（条件付き統合）
Cross-Attentionでは $Q$ を片方の入力（例：テキスト）、$K,V$ を別の入力（例：構造やスペクトル）から作る。

$$
\mathrm{CrossAttn}(X^{(A)},X^{(B)})=\mathrm{softmax}\!\left(\frac{(X^{(A)}W^Q)(X^{(B)}W^K)^\top}{\sqrt{d_k}}\right)(X^{(B)}W^V)
$$

材料科学では、以下のような「マルチモーダル統合」に相当する。
- 文献テキスト（合成条件や特徴語） ↔ 組成・構造・物性
- スペクトル ↔ 相ラベル ↔ 構造候補
- 計算（DFT特徴） ↔ 実験（スペクトルや顕微鏡像）

## 4. Multi-Head Attention：複数の観点を並列に学習する
Multi-Headでは、$h$ 個のヘッドで異なる射影を行い、

$$
\mathrm{head}_j=\mathrm{Attn}(XW^Q_j, XW^K_j, XW^V_j),\quad
\mathrm{MHA}(X)=\mathrm{Concat}(\mathrm{head}_1,\dots,\mathrm{head}_h)W^O
$$

とする。直感的には「見るべき相互関係の型」を複数同時に獲得する仕組みである。

材料の観点では、例えば
- あるヘッドは近接相互作用（局所環境）を拾い
- 別ヘッドは長距離相関（中距離秩序、弱い副ピーク、条件依存）を拾う
といった分担が起こりうる（ただし、分担の解釈は慎重であるべきである）。

## 5. 位置情報と対称性
Self-Attention自体は「トークン集合の入れ替え」に対して等価になり得るため、系列や空間の意味付けには位置情報が必要となる。材料科学では、単なる位置埋め込みよりも「物理的対称性」に沿った符号化が重要である。

材料で頻出の要件
- 並進・回転・反射への不変性（あるいは等変性）
- 原子の入れ替え（同種原子の置換）に対する不変性
- 周期境界条件（PBC）
- 距離・角度・配位の妥当性

典型的な組み込み方針
- スペクトル：区間パッチ + 位置エンコーディング（2θやエネルギー軸）
- 結晶構造：距離・角度・格子（分率座標）などを用いた相対位置表現、あるいはGNN/等変性モデルとのハイブリッド
- 組成：元素トークンに濃度（連続値）を付与し、集合として扱う（順序は任意である）

## 6. Attentionを「物理的に意味のある参照」に制限する
Attentionは全結合参照になりやすいが、マスクにより参照可能範囲を制御できる。

マスクの例
- パディングマスク：可変長入力の無効部分を参照しない
- 因果マスク：自己回帰生成で未来を見ない
- 近傍マスク：原子トークンで一定距離以内のみ参照（計算量低減と物理帰納バイアスの付与）
- 相補情報マスク：測定条件が異なるデータを混ぜない、などドメイン制約

## 7. トークン設計の対応表
| 入力データ | トークンの単位 | 代表的な付与情報 | Attentionで学ばせたい関係 |
|---|---|---|---|
| 組成 | 元素 | 濃度、周期表特徴（半径など） | 元素間相互作用、置換効果の非線形性 |
| 結晶・原子配置 | 原子（または局所環境） | 元素種、近接距離、角度、分率座標、格子 | 局所環境と中距離秩序、PBC下の相関 |
| XRD/XAFS/XPS等 | 軸方向の区間パッチ、またはピーク列 | 位置、強度、幅、バックグラウンド | 主ピークと副ピークの整合、広域パターン |
| 顕微鏡像 | 画像パッチ | パッチ位置、スケール | 欠陥・組織の長距離相関、特徴部位の抽出 |
| 文献テキスト | サブワード | ドメイン語彙、単位、条件表現 | 物質名・条件・物性の関係抽出、知識埋め込み |

## 8. Attention重みの扱い方
Attention重み（行列 $\mathrm{softmax}(\cdot)$）は「モデルが参照した割合」を示すが、因果を保証するものではない。材料科学では、以下の姿勢が実用的である。

- スペクトル：注意マップが副ピークや肩に集中しているかを“仮説生成”に使う
- 構造：特定原子間の注意が強くても、対称性や表現のバイアスの可能性を検討する
- 併用：勾配ベース、摂動（ピーク除去・局所マスキング）、部分入力での頑健性評価と併用する

## 9. 計算量とスケーリング
標準Attentionは系列長 $L$ に対して $O(L^2)$ の計算・メモリを要しやすい。材料では
- 高分解能スペクトル（大きい $L$）
- 大規模原子系（原子数が大きい $L$）
がボトルネックになりやすい。

代表的な回避策
- パッチ化・ピーク列化（スペクトルのトークン数を減らす）
- 近傍制限（局所マスク、疎Attention）
- 階層化（粗視化→詳細の二段）
- ハイブリッド（GNNで局所、Transformerで大域）

## まとめ
- Attention機構は、キー・クエリ・値による類似度計算と重み付き和により、入力内の相互関係を学習する汎用機構である。
- 材料科学への適用では、モデル選択以上に「トークン設計」「対称性・PBCの組み込み」「マスク」「評価設計」が性能と妥当性を支配する。
- Attention重みは解釈の手掛かりにはなるが因果の証明ではないため、摂動評価や物理整合性チェックと併用するのが実務的である。
