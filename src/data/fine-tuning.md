# 機械学習ポテンシャルの転移学習

機械学習ポテンシャル（ML potential, MLIP）の転移学習（ファインチューニング）とは、既存の学習済みポテンシャル（基盤モデル、ユニバーサルモデル）を出発点にして、研究対象の材料系・相・温度圧力条件・欠陥や表面など、特定用途のデータで追加学習し、精度と安定性を上げる手法である。

- 対象が特殊（元素組合せ、磁性、強相関、欠陥・界面、反応など）で、汎用モデルの誤差が目立つ
- 高精度DFT（ハイブリッド、厳密なU値、SOC、密なk点）や実験整合を狙って、参照レベルを引き上げたい
- MDを回すと不安定（非物理的な反発やスパイクが出る）なので、対象領域を補強したい
- 小規模データでも、事前学習済み表現を活かして効率よく適応したい

## 参考ドキュメント
- MACE ドキュメント: Fine-tuning Foundation Models
  - https://mace-docs.readthedocs.io/en/latest/guide/finetuning.html
- CHGNet 公式リポジトリ: Model Training / Fine-tune
  - https://github.com/CederGroupHub/chgnet
- MatGL チュートリアル
  - https://matgl.ai/tutorials.html


## 1. ファインチューニングの難しさ
1) 分布ずれ（distribution shift）
- 学習済みモデルが見た構造分布（結晶・組成・温度・ひずみ・欠陥）と、あなたが使う構造分布が違うと、外挿になり誤差が急増する。

2) 破滅的忘却（catastrophic forgetting）
- 追加学習だけに寄せると、もともと得意だった領域の性能が落ちることがある。
- 対策として、過去データの一部を混ぜて学習（replay）したり、マルチヘッド化したり、層を凍結する。

3) 参照データの一貫性
- 同じ“DFT”でも、汎関数・U・スピン設定・カットオフ・k点・補正（エネルギー補正）で「正解」が変わる。
- 基盤モデルのデータ生成条件と整合しないラベルを混ぜると、可視化上は学習しても予測が破綻しやすい。


## 2. 何を学習するのか：MLポテンシャルの数式

多くのMLIPは、原子配置 R = {r_i} から全エネルギー E(R) を予測し、そこから力と応力を導く。

エネルギー
$$
E_{\text{ML}}(R) \approx E_{\text{DFT}}(R)
$$

力（エネルギーの勾配）
$$
\mathbf{F}_i = -\frac{\partial E}{\partial \mathbf{r}_i}
$$

応力（代表的にはビリアルに関連）
$$
\sigma \leftrightarrow \frac{\partial E}{\partial \varepsilon}
$$

学習では、エネルギーだけでなく「力」「応力」も同時に合わせると、少ない構造数でも情報量を稼げてMDが安定しやすい。


## 3. 損失関数（loss）の標準形と重み付け

典型的な同時学習（エネルギー・力・応力）の損失関数は次の形になる。

$$
\mathcal{L} = w_E \, \mathrm{MSE}(E_{\text{ML}}-E_{\text{ref}})
+ w_F \, \frac{1}{3N}\sum_{i=1}^{N}\lVert \mathbf{F}_{i,\text{ML}}-\mathbf{F}_{i,\text{ref}}\rVert^2
+ w_\sigma \, \mathrm{MSE}(\sigma_{\text{ML}}-\sigma_{\text{ref}})
+ \lambda \lVert \Theta \rVert^2
$$

- w_E, w_F, w_σ: 目的に応じて調整する重み
- Θ: 学習パラメータ（正則化で過学習を抑える）

材料用途での目安（考え方）
- MD安定性・拡散・振動を重視するなら w_F を厚くする
- 相安定性（相図、形成エネルギー）を重視するなら、相対エネルギーを重視したデータ設計と w_E の扱いが重要
- 弾性・ひずみ応答を重視するなら、応力やひずみ系列データを入れて w_σ を意識する


## 4. ファインチューニングの方式

方式A: 全層を更新（naive fine-tuning）
- 長所: 最も自由度が高い
- 短所: 忘却が起きやすい、少データで過学習しやすい

方式B: 一部層を凍結して更新（freeze/unfreeze）
- 長所: 少データで安定しやすい、忘却を抑えやすい
- 短所: 表現能力の上限が出る場合がある

方式C: replay（元データの一部を混ぜる）
- 長所: 忘却に強い、全体性能を保ちやすい
- 短所: 学習コストが増える

方式D: multihead（基盤の表現は共有し、出力ヘッドを分ける）
- 長所: 基盤モデルの汎用性を保持しつつ、用途特化できる
- 短所: 実装と運用が少し複雑（どのヘッドを使うか等）

MACEでは、これらを意識したファインチューニング手順や replay/multihead の設計がドキュメントとして整理されている。


## 5. データ作成

### 5.1 どんな構造を集めるか

最低限そろえたい構造分布（例）
- 平衡構造に加えて、ひずみ（±数%）、体積変化、欠陥、表面、界面
- MDで訪れる構造（NVT/NPT/AIMDのスナップショット）
- 反応や拡散を扱うなら、遷移状態近傍や高エネルギー構造も含める

重要
- 目的が「0 Kの相安定性」なのか「有限温度MDの安定性」なのかを先に決めて、構造分布を設計する

### 5.2 DFTラベルの整合

注意が必要な混在
- GGA と GGA+U を混ぜる
- SOCあり/なしを混ぜる
- 磁気状態（初期磁気モーメント、収束したスピン状態）が混ざる
- カットオフやk点が粗くて力が不安定な計算が混ざる

運用の定石
- 参照計算条件を固定し、同一条件でラベルを作る
- どうしても混在するなら、明示的に別タスクとして扱う（multihead等）か、補正戦略を設計する

### 5.3 学習・検証の分割

時系列データ（MD）では、連続スナップショットをランダムに分けるとリークする。
推奨
- 軌道（trajectory）単位でtrain/val/testを分ける
- あるいは温度・組成・欠陥タイプなど条件単位で分け、外挿性能も測る


## 6. 評価指標

### 6.1 基本の誤差（テストセット）
- エネルギー RMSE（eV/atom）
- 力 RMSE（eV/Å）
- 応力 RMSE（GPa）

### 6.2 物理チェック
- 構造最適化：格子定数、原子位置、対称性が妥当か
- 弾性：小ひずみで応力が滑らかに出るか、弾性定数が妥当か
- フォノン（安定性）：虚数モードが不自然に増えないか
- MD安定性：NVEでエネルギーが暴れない、NVTで温度制御が破綻しない
- 欠陥形成エネルギー、表面エネルギーなど、研究目的の物性が再現できるか


## 7. 実行ワークフロー

ステップ1: 既存基盤モデルで試走
- 目的のMD/構造最適化を実際に回し、破綻例や誤差の傾向を特定する

ステップ2: 追加データ設計
- 破綻する領域（高ひずみ、欠陥近傍、高温、界面など）を重点的にDFTで追加

ステップ3: ファインチューニング
- 方式（全層/凍結/replay/multihead）を選び、学習

ステップ4: 物理的バリデーション
- テスト誤差＋物理チェック（MD安定性、弾性、フォノンなど）

ステップ5: アクティブラーニング（必要に応じて反復）
- MLIPでMD → 不確か/異常構造を抽出 → DFT追加 → 再学習


## 8. 注意点

失敗1: 低RMSEなのにMDが破綻する
- 対策: 力の学習を厚くする、反発領域（短距離）や高ひずみ構造を追加、外挿検知を導入

失敗2: 目的物性（形成エネルギー差など）が合わない
- 対策: 相対エネルギーが効くデータ設計（複数相・競合相を同時に入れる）、ラベルの一貫性を再点検

失敗3: ファインチューニング後に汎用性が落ちる
- 対策: replay、multihead、層凍結、学習率を下げる、早期終了

失敗4: データリークで評価が過大
- 対策: trajectory単位/条件単位 split を徹底


## 9. 代表的なソフトウェア

MACE
- fine-tuning と replay/multihead の公式ガイドがある
- https://mace-docs.readthedocs.io/en/latest/guide/finetuning.html
- https://mace-docs.readthedocs.io/en/latest/guide/multihead_finetuning.html

CHGNet
- 学習・ファインチューニングの入口がまとまっている
- https://github.com/CederGroupHub/chgnet
- https://chgnet.lbl.gov/

MatGL / M3GNet系
- チュートリアルから学習の流れを追える
- https://matgl.ai/tutorials.html