# 物理インフォマティクスとPINNs

物理インフォマティクスは、物理法則で制約されたモデルとデータ駆動モデルを統合し、予測・同定・設計を同じ数学的枠組みで扱うための考え方である。とくにPINNsを中心とする物理情報付き学習は、連続体場からミクロ物性までを「方程式とデータの両方」に整合させて推定できる点が核である。

## 参考ドキュメント
- MathWorks: Physics-Informed Neural Networks (PINNs) とは（日本語）
  https://jp.mathworks.com/discovery/physics-informed-neural-networks.html
- NTTアドバンステクノロジ: 物理法則を深層学習に組み込むPINN（Physics-informed Neural Networks）（日本語）
  https://www.ntt-at.co.jp/column/pinn/
- Karniadakis ほか: Physics-informed machine learning（English, Nature Reviews Physics）
  https://www.nature.com/articles/s42254-021-00314-5

## 1. 物理インフォマティクスとは何か

物理インフォマティクス（Physics Informatics）とは、支配方程式・保存則・対称性・エネルギー原理などの物理構造を、統計・機械学習と同時に扱う枠組みである。データだけでは外挿が弱く、数値モデルだけでは未知パラメータや計算量が支配的になる状況に対し、両者の長所を同時に引き出すことを目的とする。

4つのインフォマティクス（材料・計測・プロセス・物理）の中で、物理インフォマティクスは「なぜその振る舞いになるか」を支える層であり、他の3つの推定・最適化に、整合性の条件や同定の制約を与える役割を持つ。

## 2. 対象とスコープ

物理インフォマティクスが扱う「物理モデル」は単一ではなく、スケールと表現が異なる複数の層からなる。重要なのは、層ごとの方程式そのものよりも、層間で受け渡す量（定数・関数・分布）と、不確かさの伝搬を設計することである。

| レイヤー | 代表モデル | 主要出力 | 上位層への受け渡し例 |
|---|---|---|---|
| 電子構造 | DFT, GW, DMFT など | エネルギー、磁気異方性、軌道寄与、弾性など | 有効定数（異方性、交換、磁歪、拡散障壁） |
| 原子スケール | MD, MC, MLポテンシャルMD | RDF、拡散、欠陥統計、相変態の核形成など | 粗視化パラメータ、局所環境指標、輸送係数 |
| メゾスケール | フェーズフィールド、マイクロ磁気（LLG） | ドメイン/組織、界面運動、損失、場分布 | 有効場、組織統計、履歴依存パラメータ |
| マクロ | 連続体、FEM、電磁・熱・力学連成 | 温度・応力・電磁場分布、デバイス応答 | 実験条件の再現、境界条件の同定、設計指針 |
| 低次元化 | ROM、有効ハミルトニアン、Landau型 | 少数自由度モデル、近似作用素 | 最適化・制御・同化のための軽量モデル |

このページでは、これらの層をまたいで共通に使える「物理情報付き学習」の枠組みとして、PINNsとその発展形を中心に整理する。


## 3. 制約付き学習としての定式化

### 3.1 方程式制約（強形式・弱形式）を損失に入れる

未知関数を $u(x)$（必要に応じて $t$ を含む）とし、支配方程式を
$$
\mathcal{F}\bigl(x, u, \nabla u, \nabla^2 u, \dots; \lambda \bigr)=0,\quad x\in\Omega
$$
境界条件を
$$
\mathcal{B}\bigl(x, u, \nabla u; \lambda \bigr)=0,\quad x\in\partial\Omega
$$
と書く。ここで $\lambda$ は材料定数や係数場（未知でも既知でもよい）である。

ニューラルネットワークで $u_\theta(x)$ を近似し、残差
$$
r_\theta(x)=\mathcal{F}\bigl(x, u_\theta, \nabla u_\theta, \dots; \lambda \bigr)
$$
を自動微分により評価する。学習は次のような総合損失で書ける。
$$
L(\theta,\lambda)=
w_{data}L_{data}(\theta)
+w_{pde}L_{pde}(\theta,\lambda)
+w_{bc}L_{bc}(\theta,\lambda)
+w_{reg}L_{reg}(\theta,\lambda).
$$

基本形として、
$$
L_{pde}(\theta,\lambda)=\frac{1}{N_r}\sum_{i=1}^{N_r}\bigl\|r_\theta(x_i)\bigr\|^2,
\quad
L_{data}(\theta)=\frac{1}{N_d}\sum_{j=1}^{N_d}\bigl\|u_\theta(\tilde{x}_j)-\tilde{u}_j\bigr\|^2
$$
などと定義する。$\{x_i\}$ は領域内のコロケーション点、$\{(\tilde{x}_j,\tilde{u}_j)\}$ は観測データである。

### 3.2 エネルギー原理（変分）として与える

多くの場の問題は、エネルギー汎関数 $\Pi[u]$ の停留条件として書ける。例えば
$$
u^\ast=\arg\min_{u\in\mathcal{V}}\Pi[u]
$$
である。PINNsでは、PDE残差の代わりに $\Pi[u_\theta]$ を直接最小化する設計も可能である。弱形式（変分形式）を用いるVPINNsは、この考え方と相性が良い。

### 3.3 制約（保存則・対称性・幾何学的拘束）を明示する

物理では $u$ が任意の関数ではなく、拘束多様体上にあることが多い。例として、単位長拘束 $|m|=1$ を持つベクトル場 $m(x,t)$ を考えると、
$$
L_{con}(\theta)=\frac{1}{N_c}\sum_{k=1}^{N_c}\bigl(|m_\theta(x_k,t_k)|^2-1\bigr)^2
$$
のように制約違反を損失として入れられる。別の方法として、正規化をパラメトリゼーションに埋め込む（球面座標など）ことで制約を満たす表現を選ぶこともできる。


## 4. PINNs：強形式ベースの基本形（順問題・逆問題）

### 4.1 順問題：場の推定としてのPINN

順問題では、係数 $\lambda$ と境界条件が与えられ、未知の場 $u$ を求める。PINNは
- 連続関数としての $u_\theta$
- 方程式残差 $r_\theta$
を同時に扱うため、格子点上の値だけでなく、場の滑らかな内挿表現を同時に得る形式となる。

一方で、数値法（FEM/FDM）と異なり、誤差評価の機構が「離散化誤差」ではなく「学習誤差」に寄るため、収束や安定性は損失設計と最適化に大きく依存する。

### 4.2 逆問題：未知パラメータ同定としてのPINN

逆問題では $\lambda$ も未知とし、観測データと整合するように $\theta,\lambda$ を同時に最適化する。
$$
(\theta^\ast,\lambda^\ast)=\arg\min_{\theta,\lambda} L(\theta,\lambda).
$$
このとき、同定できるかどうか（同定性）は、観測の配置、境界条件の多様性、モデルの可観測性に依存する。異なる $\lambda$ が同じ観測を再現する場合があり、その場合は推定が一意に定まらない。

### 4.3 多物理連成の書き方

連成系は、未知関数の組 $u=(u^{(1)},u^{(2)},\dots)$ として
$$
\mathcal{F}^{(k)}(x,u,\nabla u,\dots;\lambda)=0,\quad k=1,2,\dots
$$
を複数課すだけで基本形は変わらない。連成が強いほど最適化が硬くなりやすく、損失のスケールやコロケーション点の割り当てが支配的になる。


## 5. PINNsの発展形：ドメイン分割・弱形式・オペレータ学習・構造化ネットワーク

PINNsの限界（高次元・多スケール・長時間・複雑形状）に対して、発展形は「どこを数値解析の知恵で補うか」により整理できる。以下では、重複を避けて体系としてまとめる。

### 5.1 ドメイン分割型：XPINNsと保存則重視の設計

空間（あるいは時空間）領域を複数の部分領域に分割し、それぞれに別のネットワークを割り当てる。領域境界では値の連続やフラックス連続などの整合条件を課す。

- 狙い
  - 局所的に難しい領域へ表現能力を集中できる
  - 並列化と相性が良い
  - 連成領域（材料領域/周辺領域など）を分けて扱いやすい

- 境界整合の基本形（例）
  部分領域 $\Omega_a,\Omega_b$ の界面 $\Gamma$ に対し、
  $$
  u_{\theta_a}(x)=u_{\theta_b}(x),\quad x\in\Gamma
  $$
  や、保存則に基づくフラックス連続
  $$
  \mathbf{n}\cdot \mathcal{G}(u_{\theta_a},\nabla u_{\theta_a})
  =
  \mathbf{n}\cdot \mathcal{G}(u_{\theta_b},\nabla u_{\theta_b}),\quad x\in\Gamma
  $$
  を損失に入れる。

XPINNsは、時空間分割まで含めて柔軟に設計できる枠組みとして整理される。

### 5.2 弱形式・変分型：VPINNsとhp-VPINNs

強形式PINNが点ごとの残差 $r_\theta(x)$ を最小化するのに対し、弱形式はテスト関数 $v$ による積分条件を課す。例えば
$$
\int_{\Omega} r_\theta(x)\,v(x)\,dx \approx 0
$$
を複数のテスト関数集合 $\{v_\ell\}$ で評価する。

- VPINNs
  - 弱形式（変分形式）で残差を定義することで、点残差よりも数値的に安定する場面がある
  - 境界層や局所特異性に対して、テスト空間の選び方が効く

- hp-VPINNs
  - $h$（領域分割）と $p$（テスト空間の多項式次数や基底の豊かさ）を組み合わせ、局所難易度に応じて表現を増強する設計である
  - 界面・先端・磁壁など、局所的に勾配が強い現象への適用が議論される

### 5.3 オペレータ学習

PINNが「1つの条件に対する解 $u$」を学ぶのに対し、オペレータ学習は「入力関数（係数場・境界条件）から解関数への写像」を学ぶ。

- 対象の写像
  係数場 $a(x)$ から解 $u(x)$ への写像を
  $$
  u=\mathcal{T}(a)
  $$
  と書き、$\mathcal{T}$ を学習する。

- 物理情報付きオペレータの基本形
  教師あり項（粗い解データなど）に加えて、予測 $u_\theta$ がPDEを満たすように残差項を加える。
  $$
  L(\theta)=L_{sup}(\theta)+\lambda L_{pde}(\theta).
  $$
  これにより、学習した写像を多数の条件に対して高速に評価できる方向性が得られる。

オペレータ学習は「多数条件の高速推論（多点スクリーニング）」に寄り、PINNは「個別条件の同定（逆問題）」に寄りやすいという構造がある。

### 5.4 構造化ネットワークと自動設計

- CNN型の物理情報付き設計
  格子上で定義された未知場をCNNで表現し、物理残差との両立を自動設計により改善する研究が進む。格子表現の強み（局所性、畳み込み）を物理制約に結びつける発想である。

- KAN型（Kolmogorov-Arnold Networks）
  KANは、重み付き和の多層変換ではなく、辺（あるいは結合）に割り当てた1次元関数の合成で表現することを特徴とする。物理情報付き学習では、残差最小化の枠組みにKANを組み合わせることで、表現と同定の性質を変える試みが議論される（PIKANsなど）。

KANの導入は「ネットワーク表現そのもの」を変える方向であり、VPINNsやXPINNsが「制約の課し方」を変える方向である点が対照的である。

### 5.5 損失のスケーリングと学習設計

発展形の多くは、根本的には次の2点を狙っている。
- 物理残差の情報が最適化で消えないようにする
- 多スケールの勾配を同時に扱えるようにする

そのために、次のような設計が併用される。
- 無次元化：変数と係数のスケールを揃える
- 損失重みの自動調整：$w_{pde},w_{bc},w_{data}$ を勾配ノルムなどから調整する
- サンプリング設計：残差が大きい領域へ点を再配分する
- 表現の工夫：Fourier特徴や位置エンコーディングにより高周波成分を表現しやすくする


## 6. 不確かさと同定性

逆問題では、最適化が解けても同定が一意に定まらないことがある。これを扱うために、確率的に $\lambda$ を扱う設計が用いられる。

### 6.1 ベイズ的な見方

事前分布 $p(\lambda)$ と尤度 $p(\tilde{u}\mid \lambda)$ を用いて
$$
p(\lambda\mid \tilde{u})\propto p(\tilde{u}\mid \lambda)p(\lambda)
$$
を評価する立場がある。PINNの枠組みでは、ニューラルネットのパラメータ不確かさや観測ノイズを組み込んだ変分推論、アンサンブルなどが検討対象となる。

### 6.2 同定性の確認

- 境界条件や外場条件を変えた複数データで $\lambda$ が一貫するか
- パラメータの相関（ある $\lambda_1,\lambda_2$ が相殺するなど）が強くないか
- 観測が状態変数のどれに感度を持つか（感度解析）

これらは数値法でも同様に現れるが、物理情報付き学習では「学習が進む＝同定できた」とは限らない点が重要である。


## 7. 使い分けの整理

### 7.1 PINNsと発展形の位置づけ

| 手法群 | 何を学ぶか | 強み | 課題が出やすい場面 |
|---|---|---|---|
| 強形式PINN | 1条件の解関数 $u(x,t)$ | 逆問題と統合しやすい、連続場表現 | 多スケール、長時間、硬い連成 |
| XPINNs（分割） | 分割領域ごとの解 | 局所難易度へ適応、並列性 | 界面条件の設計が支配的 |
| VPINNs（弱形式） | 変分条件を満たす解 | 安定性が改善する場面 | テスト空間設計に依存 |
| hp-VPINNs | 分割＋高次テスト空間 | 局所特異性へ集中 | 実装と計算が重くなる |
| オペレータ学習 | 条件→解の写像 $\mathcal{T}$ | 多数条件の高速推論 | 高品質な学習集合が必要 |
| KAN型（PIKANs） | 1次元関数合成による表現 | 表現様式を変更できる | 適用条件の見極めが必要 |

### 7.2 数値法（FEM等）との関係

| 観点 | FEM/FDM | PINN系 |
|---|---|---|
| 近似の単位 | メッシュ上の離散自由度 | 連続関数近似（NN） |
| 誤差の主要因 | 離散化誤差＋線形/非線形解法誤差 | 最適化誤差＋表現誤差＋サンプリング |
| 境界条件 | 強制しやすい | 損失または表現で課す |
| 逆問題 | 別途最適化が必要 | 同じ枠組みに統合しやすい |
| 形状・材料不連続 | メッシュ設計で扱う | 分割や界面損失が重要 |


## 8. まとめ

物理インフォマティクスは、方程式・保存則・エネルギー原理とデータを同じ最適化問題に統合し、順問題・逆問題・多条件推論を連続的に扱う枠組みである。PINNsはその基本形として強い表現力を持つ一方、多スケールや長時間の難しさが顕在化しやすく、XPINNs・VPINNs・hp-VPINNs・オペレータ学習・KAN型などの発展が「制約の課し方」と「学ぶ対象（解か写像か）」の両面から進んでいるのである。


## 関連研究
- PINN入門: 物理法則を組み込んだAIで微分方程式を解く（日本語）
  https://www.codemajin.net/introduction-to-physics-informed-neural-network/
- From PINNs to PIKANs: recent advances in physics-informed machine learning（English, Springer）
  https://link.springer.com/article/10.1007/s44379-025-00015-1
- XPINNs（時空間ドメイン分割PINNの系統）
  https://www.osti.gov/biblio/1659363
- VPINNs（弱形式PINN）
  https://link.springer.com/article/10.1007/s10915-022-01950-4
- hp-VPINNs（h-refinementとp-refinementを組み合わせたVPINNs）
  https://dblp.org/rec/journals/corr/abs-2003-05385.html
- Automated design for physics-informed modeling with convolutional neural networks（English, Nature/Communications Physics系）
  https://www.nature.com/articles/s42005-025-02414-5
- PIKANs: Physics-Informed Kolmogorov-Arnold Networks: A New Perspective on Physics-Informed Machine Learning（English, arXiv）
  https://arxiv.org/abs/2408.06650
- KAN（実装例）
  https://github.com/KindXiaoming/pykan
