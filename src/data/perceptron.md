# パーセプトロン（Perceptron）

パーセプトロンは、線形分離可能な2値分類を行う最も基本的なニューラルネットワークであり、線形分類器として理解できる。材料科学では、組成・構造記述子・スペクトル特徴量などを入力とする物性/相の2値判定のベースラインとして重要である。

## 参考ドキュメント
- Rosenblatt, F. The perceptron: a probabilistic model for information storage and organization in the brain (1958)
  https://pubmed.ncbi.nlm.nih.gov/13602029/
- Minsky, M. and Papert, S. Perceptrons: An Introduction to Computational Geometry (MIT Press)
  https://mitpress.mit.edu/9780262630221/perceptrons/
- 電子情報通信学会 知識ベース（IEICE HBKB）「ニューラルネットワーク」章（日本語PDF）
  https://www.ieice-hbkb.org/files/S3/S3gun_04hen_01.pdf


## 1. 位置づけ
パーセプトロンは、深層学習以前からある単層の学習機械であり、概念的には次のように整理できる。

- 入力：固定長ベクトル（記述子、特徴量）
- モデル：線形結合＋しきい値関数（2値出力）
- 学習：誤分類したときだけ重みを更新するオンライン学習が基本である
- 表現能力：線形分離可能な問題に限られる

材料系タスクで言えば、相A/相B、合成成功/失敗、磁性の有無、安定/不安定といった2値の粗い判定を、まず線形境界で切る基準モデルとして使う場面が多い。

## 2. モデル定義：線形分類器としてのパーセプトロン
入力ベクトルを $x\in\mathbb{R}^d$、重みを $w\in\mathbb{R}^d$、バイアスを $b\in\mathbb{R}$ とする。
スコア（活性）を
$$
a = w^\top x + b
$$
と定義し、2値出力はしきい値関数で与えるのが基本である。
$$
y =
\begin{cases}
1 & (a \ge 0) \\
0 & (a < 0)
\end{cases}
$$

幾何学的には、$w^\top x + b = 0$ が決定境界（超平面）であり、入力空間を2つの半空間に分割するモデルである。
材料記述子（平均電気陰性度、原子半径差、価電子数、局所構造統計量など）が与えられたとき、それらの線形結合でクラスを判定する最小モデルである。

## 3. 学習則（誤り訂正学習）
学習データを $\{(x^{(i)}, t^{(i)})\}$ とし、教師信号を $t\in\{0,1\}$ で与える場合、誤分類したサンプルに対してのみ更新する形が典型である。

- 予測：$y^{(i)} = \mathbb{I}[w^\top x^{(i)} + b \ge 0]$
- 更新（誤分類時）：
$$
w \leftarrow w + \eta\,(t^{(i)}-y^{(i)})\,x^{(i)},\quad
b \leftarrow b + \eta\,(t^{(i)}-y^{(i)})
$$
ここで $\eta>0$ は学習率である。

別表現として、教師信号を $t\in\{-1,+1\}$、出力を $y=\mathrm{sign}(w^\top x + b)$ とする場合は、誤分類（$t(w^\top x + b)\le 0$）のとき
$$
w \leftarrow w + \eta\, t\, x,\quad b \leftarrow b + \eta\, t
$$
と書くこともできる。

この学習はオンライン学習（入力が来るたびに更新）として実装しやすく、初期の識別学習の基本形である。

## 4. 線形分離可能性と収束（パーセプトロン収束定理）
パーセプトロンが「うまくいく」条件は、クラスが線形分離可能であることである。

線形分離可能とは、ある $(w^*, b^*)$ が存在して
$$
t^{(i)}(w^{*\top}x^{(i)} + b^*) > 0
$$
が全サンプルで成り立つことである（ここでは $t\in\{-1,+1\}$ の表現を用いた）。

この条件が満たされる場合、パーセプトロン学習は有限回の誤り更新で停止し、全訓練データを正しく分類する重みへ到達することが示される。
また、マージン $\gamma$ と入力ノルム上界 $R$ を用いると、誤り更新回数 $M$ は概ね
$$
M \le \left(\frac{R}{\gamma}\right)^2
$$
のように上から抑えられる形で議論される（定数や定義は導入の流儀で変わる）。

材料データでは、測定ノイズや相混在、ラベル曖昧性により厳密な線形分離が崩れやすく、収束しても汎化が保証されない点に注意が必要である。

## 5. 限界
### 5.1 非線形分離問題（XORなど）が解けない
単層パーセプトロンは線形境界しか持てないため、XORのような線形分離不可能な問題は表現できない。
この限界の認識は、多層化（中間層の導入）や別の学習枠組みの発展につながった。

### 5.2 確率出力を持たない
出力がしきい値で離散化されるため、予測確率や不確かさを直接与えるモデルではない。
材料設計で重要な不確かさ推定（測定誤差や外挿の危険度）には別設計が必要である。

### 5.3 ノイズ・ラベル誤りに弱い場合がある
線形分離できないデータに対しては更新が振動し、停止しない（あるいは性能が安定しない）ことがある。
実務では、正則化付き線形モデル（ロジスティック回帰、線形SVM）や確率的勾配法（損失関数を明示）に移行することが多い。

## 6. 多層パーセプトロン（MLP）との関係
パーセプトロンは単層（線形＋しきい値）であるのに対し、MLPは中間層を持ち、非線形活性を介して
$$
x \rightarrow \sigma(W_1 x) \rightarrow \sigma(W_2 \sigma(W_1 x)) \rightarrow \cdots
$$
のように非線形関数近似が可能である。
XORが解けないという単層の限界は、中間層を導入することで回避できるという意味で、パーセプトロンは深層学習の最小原型である。

## 7. 材料科学での使いどころ
### 7.1 ベースラインとしての価値
- まず線形境界でどこまで分類できるかを測ることで、記述子の線形分離性を点検できる
- 複雑なモデル（GNN、Transformer、CNN）を使う妥当性を説明しやすくなる

### 7.2 入力設計の例
- 組成系：元素分率＋物性表から作った統計量（平均、分散、最大差）
- 構造系：配位数、最近接距離統計、対称性由来スカラー量
- スペクトル系：ピーク位置・幅・強度比、低次元圧縮（PCAなど）のスコア

### 7.3 最小運用の注意
- 特徴量スケールを揃える（標準化など）と学習が安定しやすい
- 近縁系混入（同一系・同一ロット由来のリーク）を避けた分割が重要である
- クラス不均衡が強い場合、単純な正解率より適切な指標（再現率、F1）を併用する

## まとめ
パーセプトロンは、線形分類を行う単層ニューラルネットワークであり、誤分類時のみ更新する単純な学習則を持つ基礎モデルである。材料科学では、組成・構造・スペクトル特徴量に対して線形分離性を点検するベースラインとして有用である一方、XORに代表される非線形分離問題を表現できないため、必要に応じてMLPや他の非線形モデルへ拡張するのが自然である。
