# 入れ子学習（Nested Learning, NL）

入れ子学習（Nested Learning, NL）は、ニューラルネットワークを「異なる更新頻度をもつ複数の学習（最適化）過程が入れ子になった系」として再定義する枠組みである。これにより、推論中の適応（in-context learning）から学習器（オプティマイザ）状態、重み更新、さらに学習則の更新までを、同一の視点で統合的に説明することを目指す。

### 参考ドキュメント
1. Introducing Nested Learning: A new ML paradigm for continual learning（Google Research Blog, 2025）
   https://research.google/blog/introducing-nested-learning-a-new-ml-paradigm-for-continual-learning/

2. Ali Behrouz, Nested Learning: The Illusion of Deep Learning Architectures（論文PDF）
   https://abehrouz.github.io/files/NL.pdf

3. （日本語）Nested Learning と HOPE アーキテクチャの解説ブログ（Zenn, 2025/11/25）
   https://zenn.dev/m_nakano_teppei/articles/5c2b9d75fadb79


## 1. なぜ「入れ子」視点が必要か

深層学習は、固定のパラメータを学習し、推論時には原則として重みを変えずに入力から出力を得る枠組みとして普及してきた。一方で、連続的に到来するデータ列（対話、ストリーミング観測、オンライン意思決定）に対しては、推論と学習が時間的に絡み合う。ここで中心問題になるのが、継続学習（continual learning）における破滅的忘却（catastrophic forgetting）である。新しい課題や分布に適応すると、以前の知識や技能が劣化する現象であり、学習の安定性・保持性・可塑性の両立が難題となる。

NLは、モデル内部にすでに存在する「時間スケールの違う更新」を明示化し、記憶と学習を更新頻度のスペクトルとして扱うことで、忘却と適応を同時に論じる道具立てを与えることを狙う。


## 2. 用語の整理：混同しやすい「nested」

「Nested」という語は機械学習で複数の意味に使われるため、ここで区別する。

| 用語 | 目的 | 入れ子の対象 | 主な出力 |
|---|---|---|---|
| Nested cross-validation（入れ子交差検証） | 汎化性能推定とハイパーパラメータ選択の分離 | データ分割（外側CVと内側CV） | 推定精度、最適ハイパーパラメータ |
| Bilevel optimization / Meta-learning | 「学習の学習」やハイパーパラメータ最適化 | 目的関数（内側最適化と外側最適化） | 初期値、学習則、ハイパーパラメータ |
| Nested Learning（本稿のNL） | モデルを多時間スケールの最適化系として統一記述 | モジュール更新（推論内更新・重み更新・学習則更新など） | 継続学習・長期記憶・高次の適応能力の設計指針 |

本稿のNLは、特に「アーキテクチャと最適化を統合されたシステムとして見る」点に特徴がある。


## 3. コンテキストフローと連想記憶

NLの議論では「コンテキスト（文脈）」を、時系列入力や観測列だけに限定しない。モデル内の任意の要素（注意機構の状態、RNNの隠れ状態、オプティマイザのモーメント、重み自体など）が参照しうる情報の流れを、広くコンテキストフローとして捉える。

さらに、NLは「学習＝有用な記憶の獲得」「記憶＝入力により引き起こされるニューラル更新」という神経心理学に基づく語彙を採り、最適化による更新を連想記憶（associative memory）の一形態として位置づける。直観的には、モデルは過去のコンテキストフローをその内部状態やパラメータへ圧縮し、その圧縮表現を通じて将来の入力に応答する、という見方である。


## 4. 数理：多時間スケール更新としてのNL

### 4.1 モジュール分解と更新頻度

NLの基本形は、モデルを複数の写像（モジュール）に分解し、それぞれが異なる頻度で更新されるとみなすことである。入力列 $x=\{\mathbf{x}_{1},\dots,\mathbf{x}_{T}\}$ に対し、$k$ 個のモジュールを逐次適用して

$$
\mathbf{y}_{t}= \mathrm{MLP}(f_{k})(\mathrm{MLP}(f_{k-1})(\cdots \mathrm{MLP}(f_{1})(\mathbf{x}_{t})))
$$

のように表す。ここで $\ell$ 番目のブロックのパラメータ $\boldsymbol{\theta}^{(f_{\ell})}$ は、毎ステップではなく $C^{(\ell)}$ ステップごとに更新されるとする。概念的に

- 大きい $C^{(\ell)}$：更新が遅い（長い時間スケールで保持されやすい）
- 小さい $C^{(\ell)}$：更新が速い（短期適応に寄与しやすい）

という解釈が成立する。更新則を一般のオプティマイザ誤差項 $f(\cdot)$ を用いて書けば、概略として

$$
\boldsymbol{\theta}^{(f_{\ell})}_{i+1}=
\boldsymbol{\theta}^{(f_{\ell})}_{i}
-\sum_{t=i-C^{(\ell)}}^{i} \eta^{(\ell)}_{t}\, f(\boldsymbol{\theta}^{(f_{\ell})}_{t};\mathbf{x}_{t})
\quad (i \equiv 0 \ \mathrm{mod}\ C^{(\ell)})
$$

のような形になる（更新しないステップでは変化なし）。このとき各モジュールは「自分に流れ込むコンテキストフローを自分のパラメータへ圧縮する」役割を担う、と解釈される。

### 4.2 連続メモリシステム（Continuum Memory System, CMS）

NLの重要な設計として、更新頻度の異なる複数の学習ブロックを並置し、記憶を「短期・長期の二値」ではなく連続的スペクトルとして構成する考え方が提示される。CMSでは、更新が速いブロックが即時適応を担当し、更新が遅いブロックが持続的な知識保持に寄与する。

この「頻度の階層化」は、破滅的忘却への耐性を高める方向に働くと説明される。直観的には、新規情報はまず高速層に入り、十分に反復され安定化した情報のみが低速層へ移る、という「時間的な分業」を作れるからである。

### 4.3 高次のin-context learning

NLでは、各レベルが固有のコンテキスト長を持ち、区間の終端で再初期化されるような構成により、高次の適応（higher-order in-context learning）を論じる。例えば、あるレベル $s$ において次レベル $s+1$ の初期状態がメタ的に学習される形として

$$
\boldsymbol{\theta}^{(f_{s+1})}_{0}
=
\arg\min_{\Phi}\ \mathbb{E}_{T\sim C^{(s)}}\left[\ell(\Theta,T;\Phi)\right]
$$

のような式で表される。ここで $C^{(s)}$ は $s$ レベルのコンテキスト長である。再初期化を伴う設計は、短期適応と長期保持の干渉を抑える方向に働きうる一方、最適化の安定性や計算量との兼ね合いが設計論点となる。


## 5. 既存深層学習の再解釈：TransformerはNLの特殊例か

NLの主張の一つは、現在「アーキテクチャの多様性」として見えている差異の一部が、実は「異なる更新周波数をもつ最適化の入れ子構造」の投影として理解できる、という点である。たとえば、Transformerブロックを上の枠組みの特殊例（$k=1$）として位置づけ、推論時の注意機構（KVキャッシュ）が高速に更新される短期メモリとして機能し、学習による重み更新がより遅い時間スケールでの記憶形成として機能するとみなす。

この視点は、次のような統合を促す。

- 推論中に変わる状態（KVキャッシュ、隠れ状態）も「更新＝記憶」として扱う
- 学習で変わる状態（重み）も同じ連続体の別の時間スケールに置く
- オプティマイザ状態（Adamの $m,v$ など）も、さらに別の時間スケールの記憶として位置づける


## 6. オプティマイザを「メモリ」として見る：更新則の圧縮

NLは、オプティマイザを単なる手続きではなく、過去の勾配情報を内部状態に圧縮し続ける連想記憶系とみなす方向を強調する。例えば勾配 $g_{t}=\nabla_{\theta}\mathcal{L}_{t}$ として、MomentumやAdamは

Momentum:
$$
\mathbf{m}_{t}=\beta \mathbf{m}_{t-1}+(1-\beta)\mathbf{g}_{t},\qquad
\boldsymbol{\theta}_{t+1}=\boldsymbol{\theta}_{t}-\eta \mathbf{m}_{t}
$$

Adam（基本形）:
$$
\mathbf{m}_{t}=\beta_{1}\mathbf{m}_{t-1}+(1-\beta_{1})\mathbf{g}_{t},\quad
\mathbf{v}_{t}=\beta_{2}\mathbf{v}_{t-1}+(1-\beta_{2})\mathbf{g}_{t}^{2},\quad
\boldsymbol{\theta}_{t+1}=\boldsymbol{\theta}_{t}-\eta \frac{\hat{\mathbf{m}}_{t}}{\sqrt{\hat{\mathbf{v}}_{t}}+\epsilon}
$$

のように、更新に必要な「過去の情報」を $(\mathbf{m}_{t},\mathbf{v}_{t})$ として保持する。NLの観点では、この保持そのものがコンテキストフローの圧縮であり、学習モジュールの一部として設計対象になる。


## 7. HOPEアーキテクチャ：自己修正メモリとCMSの結合

NLに基づく具現化の一つとして、HOPE（Hierarchically Optimized Persistent Entity）と呼ばれる学習モジュール設計が提示されている。要点は次である。

1. 自己修正型の長期メモリ（Titans系の考え方を含む）により、推論中の文脈に合わせて「記憶の書き込み規則」自体を表現力高くする
2. CMSにより、更新頻度の異なる複数の記憶ブロックへ分配し、持続的保持と適応の両立を狙う

NL論文では、CMSが「大きい容量だが更新則は単純」である方向に適し、自己修正型メモリが「容量は小さいが更新則が複雑で表現力が高い」方向に適する、と対比され、両者が相補的であると述べられる。


## 8. 継続学習との関係：忘却をどう扱うか

破滅的忘却を抑える研究には、正則化（EWCなど）、リプレイ、パラメータ分割、動的拡張など多様な系統がある。NLはこれらと競合するというより、「更新頻度のスペクトル」という軸で設計を再整理し、次の見通しを与える。

- 高頻度更新：局所文脈や短期適応を担う（変化に強い）
- 低頻度更新：持続的知識を担う（忘却に強い）
- 中間頻度：両者の橋渡しとして、安定化と適応を調停する

このとき、忘却は「単一の重みにすべてを詰め込む」ことから生じる干渉として理解され、時間スケールを分けることで干渉を低減できる、という説明になる。


## 9. 研究上の論点：何が難しく、何が未確定か

NLは統一的な見方を与える一方、検証すべき論点も多い。例えば以下が挙げられる。

- 階層数 $k$ と更新間隔 $C^{(\ell)}$ の設計原理（タスク・データ分布・モデル規模との関係）
- 再初期化やメタ最適化の安定性（学習の発散、勾配ノイズ、計算資源）
- 「更新則の表現力」と「記憶容量」のトレードオフの定量化
- 既存の長文脈技術（外部メモリ、検索拡張、圧縮メモリ）との関係整理

これらは、概念としてのNLを、再現可能な設計指針へ落とし込むための主要課題である。


## 10. 応用例

NLは、入力分布が時間とともに変化する設定で特に重要になる。

- 対話型モデル：ユーザ適応と一般知識保持の両立
- ストリーミング科学データ：観測条件のドリフト下での継続的同定
- ロボティクス・制御：環境変化への高速適応と安全な保持
- 時系列予測：外乱・季節性変化を含む長期運用での保持と更新

ここでNLの価値は、個別手法の羅列ではなく、「どの情報をどの時間スケールで保持するか」という設計言語を提供する点にある。


## まとめと展望

入れ子学習（NL）は、モデル内部の更新を多時間スケールの入れ子最適化として統一的に捉え、推論内適応・重み更新・オプティマイザ状態・学習則の更新までを「記憶の連続体」として説明する枠組みである。HOPEのような設計は、表現力の高い自己修正メモリと、更新頻度を分割したCMSを組み合わせることで、継続学習における保持と適応の両立へ接近する試みとして位置づけられる。

今後は、更新頻度設計の原理化、再現性の高い公開実装と検証、既存の長文脈・外部メモリ・検索拡張との統合、さらに理論的な安定性解析が進むことで、NLが「説明枠組み」から「設計科学」へ展開していくことが期待される。



### 参考（その他）
- Titans: Learning to Memorize at Test Time（arXiv）
  https://arxiv.org/abs/2501.00663
- Titans and MIRAS: Advancing Long-Context Language Models（Google Research Blog, 2025）
  https://research.google/blog/titans-and-miras-advancing-long-context-language-models/
- Overcoming catastrophic forgetting in neural networks（EWC, PNAS 2017）
  https://www.pnas.org/doi/10.1073/pnas.1611835114
- Continual lifelong learning with neural networks: A review（Neural Networks 2019）
  https://doi.org/10.1016/j.neunet.2019.01.012
