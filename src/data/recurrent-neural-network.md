# リカレントニューラルネットワーク（RNN）入門（材料科学・MI向け）

RNN（Recurrent Neural Network）は、時系列・系列データを「過去の情報（状態）」として保持しながら処理するニューラルネットワークです。材料科学では、スペクトル（XRD/XAFS等）、プロセスログ、in-situ計測の時系列、磁気雑音波形、MD軌跡など「順序に意味があるデータ」に強みがあります。

## ねらい
- RNNの最小数式（状態更新・学習）を理解する
- LSTM/GRUが必要になる理由（勾配消失/爆発）を掴む
- 材料データでの使い所と注意点（リーク・分割・前処理）を把握する


## 参考ドキュメント
- Elman, J. L. “Finding Structure in Time” (1990)（RNNの古典的論文）
  https://papers.baulab.info/papers/Elman-1990.pdf
- 筒井ほか「Current Trends on Deep Learning Techniques Applied in ...」鉄と鋼（J-STAGE, 2023）（RNN/LSTMの概説を含む）
  https://www.jstage.jst.go.jp/article/tetsutohagane/109/6/109_TETSU-2022-098/_html/-char/en
- 慶應義塾大学スライド「機械学習基礎 再帰型ニューラルネット」(SpeakerDeck, 2021)
  https://speakerdeck.com/keio_smilab/keio-univ-intro-to-ml-07-rnn


## 1. RNNとは

時刻（または系列インデックス）t の入力を $x_t$、隠れ状態（内部メモリ）を $h_t$ とすると、基本形（Simple RNN）は

$$
h_t = \phi\left(W_{xh}x_t + W_{hh}h_{t-1} + b_h\right)
$$

出力 $y_t$ を出す場合は

$$
y_t = \psi\left(W_{hy}h_t + b_y\right)
$$

- $W_{xh}, W_{hh}, W_{hy}$: 重み行列
- $b_h, b_y$: バイアス
- $\phi, \psi$: 活性化関数（tanh/ReLU/Softmaxなど）

「同じ重みを全時刻で共有する」ことで系列長が変わっても処理できます。

### 1.1 展開（unroll）の直感
RNNは、時間方向に展開すると「深いネットワーク」に見えます（時刻ごとに同じブロックが繰り返される）。


## 2. 学習：BPTT（時間方向の誤差逆伝播）

RNNは一般に、時間方向へ誤差を逆伝播する BPTT（Backpropagation Through Time）で学習します。
代表的な目的関数（回帰の例）は

$$
\mathcal{L} = \frac{1}{T}\sum_{t=1}^{T}\left(y_t-\hat{y}_t\right)^2
$$

### 2.1 勾配消失・勾配爆発（RNNの根本課題）
展開した深いネットワークで、同じヤコビアン（微分）の積が何度も掛かるため、
- 微分の大きさが 1 未満に偏る → 勾配消失（長期依存が学べない）
- 1 超に偏る → 勾配爆発（学習が不安定）
が起きやすくなります。


## 3. LSTM / GRU：なぜ「ゲート付きRNN」が主流なのか

### 3.1 LSTM（長短期記憶）
LSTMは「セル状態 $c_t$」とゲート（入力・忘却・出力）で、情報の保持/破棄を制御します（概念式）：

$$
c_t = f_t \odot c_{t-1} + i_t \odot \tilde{c}_t
$$

- $f_t$: 忘却ゲート（どれだけ過去を保持するか）
- $i_t$: 入力ゲート（新情報をどれだけ入れるか）
- $\odot$: 要素ごとの積

長期依存を扱いやすくする代表的な設計です。

### 3.2 GRU（ゲート付きリカレントユニット）
GRUはLSTMを簡略化したゲート構造で、実務では「まずGRUを試す」ことも多いです（モデルが軽い場合がある）。


## 4. 材料科学でのユースケース

### 4.1 スペクトルを「系列」として扱う（XRD/XAFS/ELNES など）
- 入力：強度列 $x_1,\dots,x_T$（角度/エネルギー軸に沿った系列）
- 目的：欠陥率推定、相・結晶性の分類、ピーク形状の回帰、in-situ変化追跡

例：シミュレーションXRDパターンから点欠陥割合をLSTMで推定する枠組みなど。

### 4.2 磁気雑音・プロセス波形（MBN、電圧/電流ログ等）
- 入力：時間波形（系列）
- 目的：応力・損傷・粒径などの推定、異常検知、品質モニタリング
- 特徴：ノイズが多いので、窓切り出し（sliding window）＋LSTM/GRUが相性良いことがある

### 4.3 微細組織の時間発展（連続観察・シミュレーション）
- 入力：時系列特徴（相分率、粒径統計、形態指標など）
- 目的：組織発展の時系列予測、プロセス条件最適化のサロゲート

### 4.4 MD軌跡の系列（学習・分類）
- 入力：原子配置から抽出した時系列特徴（RDFの要約、局所秩序パラメータ、拡散係数の時系列など）
- 目的：相転移の予兆検出、緩和過程の分類、メタ安定状態の同定支援


## 5. どのRNNを選ぶ？

| 目的 | 推奨モデル | 理由 |
|---|---|---|
| 短い系列・まず動かす | Simple RNN | 最小構成で理解用（実務は不安定になりがち） |
| 長期依存を扱う | LSTM | 勾配消失に強い設計が基本思想 |
| 軽量・高速にしたい | GRU | パラメータが少なめで収束しやすい場合 |
| 前後文脈が効く | BiLSTM / BiGRU | 両方向の文脈を利用（オフライン解析向き） |
| 入出力が系列（変換） | Encoder–Decoder（Seq2Seq） | スペクトル→スペクトル、信号→信号などに応用 |


## 6. 前処理と分割

### 6.1 前処理の基本
- スペクトル：ベースライン補正、正規化（pre-edge / post-edge等）、リサンプリング
- 波形：帯域フィルタ、標準化、窓切り出し（$T$ を揃える）
- 欠損：補間か、マスク（モデルに欠損を知らせる）

### 6.2 データ分割（リーク対策）
材料では「同一試料・同一ロット・同一計測日の揺れ」が強いので、ランダム分割だけで良く見えることがあります。
- 組成で分割（未知組成への汎化）
- 試料ID/ロットで分割（同一系列の混入防止）
- 時系列で分割（将来予測を真に評価）


## 7. RNNの解釈
- 時刻ごとの重要度：勾配ベースの寄与（saliency）、Integrated Gradients
- 入力窓のどこが効いたか：摂動（masking）で性能低下を見る
- 出力に効く時刻の可視化：attention付きRNN（ただし「説明＝因果」ではない点に注意）


## まとめ
RNNは、材料研究で頻出する「スペクトル・波形・プロセスログ・in-situ時系列」を扱うための基本モデルです。実務ではSimple RNNより、長期依存と学習安定性に優れるLSTM/GRUが中心になります。まずは分割（リーク防止）と前処理を固め、短い系列でベースラインを作った上で、必要に応じてBiRNNやSeq2Seqへ拡張してください。