# 特徴量重要度・寄与分解（SHAP, LIMEなど）

特徴量重要度・寄与分解は、モデル予測を入力特徴量の寄与として分解し、なぜその予測になったかを局所（個別サンプル）または大域（全体傾向）で説明する枠組みである。材料科学では、組成・構造記述子・スペクトル特徴・画像特徴が物性や相判定にどう効いたかを定量化し、設計則や機構仮説へ接続する目的で用いられる。

## 参考ドキュメント
- Lundberg and Lee, A Unified Approach to Interpreting Model Predictions (SHAP)
  https://arxiv.org/abs/1705.07874
- Ribeiro et al., "Why Should I Trust You?": Explaining the Predictions of Any Classifier (LIME)
  https://arxiv.org/abs/1602.04938
- DataRobot Japan: SHAPを用いて機械学習モデルを説明する（日本語）
  https://www.datarobot.com/jp/blog/explain-machine-learning-models-using-shap/


## 1. 説明可能AI（XAI）
特徴量重要度は、主に次の2軸で整理できる。

1) どの範囲を説明するか
- 大域的（global）：モデル全体で重要な特徴量は何か
- 局所的（local）：特定サンプルの予測に効いた特徴量は何か

2) どのモデルに依存するか
- モデル非依存（model-agnostic）：任意のモデルを外から説明（例：LIME、Kernel SHAP）
- モデル特化（model-specific）：構造を利用して高速・安定に説明（例：Tree SHAP、線形モデルの厳密SHAP）

材料データでは、データ数が少なく分布が偏りやすいこと、特徴量間の相関が強いことが多いため、局所説明を大量に集約して大域的傾向を読む運用と、外挿評価（未知組成域・未知条件域）を前提にした説明の検証が重要である。

## 2. 代表手法の直感
### 2.1 SHAP（Shapley Additive Explanations）
協力ゲーム理論のシャープレイ値に基づき、予測値を「ベースライン＋特徴量の加法的寄与」に分解する考え方である。
- 強み：寄与の加法性が明確で、局所説明を集約して大域的理解につなげやすい
- 弱み：一般には計算が重く、特徴量依存（相関）をどう扱うかで解釈が変わり得る

### 2.2 LIME（Local Interpretable Model-agnostic Explanations）
説明したい点の近傍で入力を摂動してデータを増やし、元モデルの出力を局所的に近似する単純モデル（多くは疎な線形モデル）を学習して説明する。
- 強み：モデル非依存で導入が容易、直感的（局所で直線近似）
- 弱み：摂動の作り方・近傍の定義に敏感で、説明が不安定になり得る

### 2.3 重要度と寄与分解の違い
- 特徴量重要度（importance）：順位付けが主目的（例：Permutation Importance）
- 寄与分解（attribution）：予測値を寄与の和として分解し、符号（増やす/減らす）まで扱う（例：SHAP、LIMEの係数）

材料設計では「どれが効くか」だけでなく「どちら向きに効くか（例：増加が強度を上げるのか下げるのか）」が重要であるため、寄与分解が有用になりやすい。

## 3. 数式

### 3.1 シャープレイ値（Shapley value）
特徴量集合を $F=\{1,\dots,M\}$、特徴量部分集合を $S\subseteq F\setminus \{i\}$ とし、特徴量 $i$ の寄与を定義する。
$$
\phi_i = \sum_{S\subseteq F\setminus \{i\}} \frac{|S|!(M-|S|-1)!}{M!}\Big( v(S\cup\{i\}) - v(S)\Big)
$$
ここで $v(S)$ は「特徴量集合 $S$ だけを使ったときのモデル出力」を表す関数であり、実務では未知特徴量の扱い（周辺化、条件付きなど）の設計が必要である。

### 3.2 SHAPの加法モデル（寄与分解）
SHAPは、説明モデルを加法形で表す。
$$
g(z)=\phi_0+\sum_{i=1}^M \phi_i z_i
$$
$z_i\in\{0,1\}$ は特徴量 $i$ を使うかどうかの指示である。予測 $f(x)$ に対し、
$$
f(x)\approx \phi_0+\sum_{i=1}^M \phi_i
$$
となるように $\phi_i$ を求め、$\phi_i$ を特徴量寄与（SHAP値）として解釈する。

### 3.3 LIMEの局所近似
LIMEは、元モデル $f$ を局所的に単純モデル $g$ で近似する最適化として書ける。
$$
\arg\min_{g\in\mathcal{G}}\ \mathcal{L}\big(f, g, \pi_x\big) + \Omega(g)
$$
$\pi_x$ は説明対象 $x$ 近傍を重くする重み、$\Omega(g)$ は単純さ（疎性など）を促す正則化である。材料データでは、摂動が物理的に不自然（あり得ない組成・記述子）になりやすく、ここが説明の品質を左右しやすい。

## 4. 出力の読み方
表形式・スペクトル特徴・画像特徴（埋め込み）などを入力とする場合、典型的に次を使う。

- 局所：waterfall / force（あるサンプルで、どの特徴が予測を押し上げ/押し下げたか）
- 大域：beeswarm（多サンプルでのSHAP値分布）と平均絶対SHAP（重要度）
- 相互作用：SHAP interaction（特徴量ペアが同時に効く効果、ただし解釈は慎重に扱う）

注意として、重要度上位であっても「相関の代理」である可能性がある。例えば、熱処理温度と粒径が強相関である場合、どちらが本質かはSHAPだけでは確定しない。

## 5. 解釈ポイント
### 5.1 組成・元素特徴（合金、化合物）
- よくある入力：元素分率、平均電気陰性度、原子半径差、価電子数、混合エンタルピーの近似量
- 注意：組成制約（総和=1）により特徴量が従属しやすい
  - 対応：基底表現（独立変数化）、無次元化、組成空間の扱いを統一する

### 5.2 構造記述子（結晶、局所環境）
- よくある入力：配位数、最近接距離、原子環境の統計、対称性指標、SOAPなどの埋め込み
- 注意：回転・並進・入れ替え不変性を満たす特徴で説明しないと、説明が物理直感とずれる

### 5.3 スペクトル（XRD/XAFS/XPSなど）
- よくある入力：ピーク位置・幅・強度比、区間パッチ特徴、PCA成分など
- 注意：ピーク同士が強相関で、1つの「相の存在」を複数特徴が重複表現しやすい
  - 対応：相の候補や物理モデル（散乱・遷移）に沿った特徴設計、ピーク集合としての説明（グルーピング）

### 5.4 画像（組織、欠陥、破面）
- Grad-CAMは空間可視化に強い一方、SHAP/LIMEは「特徴量（ピクセル/スーパー・ピクセル/埋め込み）」の寄与として説明することが多い
- 注意：スケールバー、撮影条件、コントラストなどがリーク源になりやすい
  - 対応：マスクテスト、領域除去、撮影条件を統制した分割で検証する

## 6. 注意点
### 6.1 データリークと近縁系混入
- 同一材料の表記揺れ、近い組成・同系統プロセスが学習/評価に跨ると、説明も見かけ上もっともらしくなる
- 対策：group split（材料ID、系、ロット、実験日など）と外挿テスト（未知組成域）を用意する

### 6.2 特徴量相関と寄与の不安定性
- SHAPもLIMEも、相関が強いと寄与の割り振りが揺れる
- 対策：特徴量をまとめた説明（クラスター化、グループSHAP）、条件付き/周辺化の設計を明示する、説明の再現性（乱数・摂動）を評価する

### 6.3 ベースライン（背景分布）の選び方
- SHAPでは背景データ（期待値の基準）が説明を決める
- 対策：材料的に意味のある基準（代表組成、代表プロセス、空気/真空など）を選ぶか、複数ベースラインで感度分析を行う

### 6.4 因果と説明の混同
- 寄与が大きいことは因果を意味しない
- 対策：介入可能な変数（組成や熱処理）での追試、摂動実験、物理モデルとの整合チェックを行う

## 7. ワークフロー
1. 目的を固定する（機構仮説、設計指針、リーク検出、品質管理のどれか）
2. 分割を先に決める（外挿評価とgroup split）
3. ベースラインと説明対象を定義する（SHAPの背景、LIMEの近傍）
4. 局所説明を多数集める（代表点・失敗例・境界例）
5. 集約して大域傾向を読む（重要度、分布、相互作用）
6. 説明の頑健性を検証する（乱数、摂動、サブサンプル、別モデルで一致するか）
7. 物理妥当性で絞る（単位、極限、対称性、既知則との整合）

## まとめ
特徴量重要度・寄与分解（SHAP、LIMEなど）は、材料モデルの予測を特徴量寄与として読み解き、設計則・機構仮説・リーク検出に接続するための中核的XAIである。材料データ特有の相関・制約・外挿課題を踏まえ、分割設計とベースライン設計、頑健性検証をセットで運用することが実用上の鍵である。
