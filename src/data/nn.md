# ニューラルネットワーク

ニューラルネットワーク（NN）は、材料の組成・構造・計測データから物性や反応性などを予測するための汎用モデルです。材料科学では「どの表現（特徴量）を入れるか」と「汎化できているか」を特に意識して使います。

- NNの最小限の数式と用語を押さえる
- 材料データをNNに入れるときの表現の選択肢を理解する
- 物性予測・ポテンシャル・計測解析の代表例を俯瞰する


## 参考ドキュメント
- Deep learning for molecules & materials（日本語）: 
  https://resnant.github.io/dmol-book-japanese/
- 材料科学における機械学習の利用と課題（日本語, J-STAGE）: 
  https://www.jstage.jst.go.jp/article/jnns/28/1/28_20/_article/-char/ja/
- Deep Learning（Goodfellow, Bengio, Courville / 英語・オンライン版）: 
  https://www.deeplearningbook.org/


## 1. ニューラルネットワークが得意なタスク
1) 回帰: $y \in \mathbb{R}$（例: 形成エネルギー、弾性定数、$M_s$、$T_C$、バンドギャップ）
2) 分類: $y \in \{1,\dots,K\}$（例: 相判定、合成可否、異常有無）
3) 表現学習: 材料を低次元ベクトル $z$ に埋め込み、類似検索やクラスタリングに使う
4) サロゲート: DFT/MD/有限要素の近似器として高速評価（逆設計・最適化にも接続しやすい）


## 2. 数式で理解するNN

### 2.1 1層（線形）＋活性化（非線形）
入力 $x \in \mathbb{R}^d$ に対して
$$
h = \phi(Wx + b)
$$
- $W$ は重み行列、$b$ はバイアス
- $\phi$ は活性化関数（ReLU, GELU, tanh など）
- 層を重ねると複雑な非線形写像を表現できる

### 2.2 目的関数（損失）
回帰（例: 平均二乗誤差）
$$
\mathcal{L}_{\mathrm{MSE}} = \frac{1}{N}\sum_{i=1}^{N}\left(y_i-\hat{y}_i\right)^2
$$
分類（例: ソフトマックス＋交差エントロピー）
$$
p_k = \frac{\exp(z_k)}{\sum_{j=1}^{K}\exp(z_j)}, \quad
\mathcal{L}_{\mathrm{CE}} = -\frac{1}{N}\sum_{i=1}^{N}\log p_{y_i}
$$

### 2.3 学習（最適化）
勾配降下法（更新式のイメージ）
$$
\theta \leftarrow \theta - \eta \nabla_{\theta}\mathcal{L}
$$
- $\theta$ は全パラメータ（$W,b$ の集合）
- $\eta$ は学習率
- 実装上はミニバッチ、Adam などの最適化手法を使うことが多い

### 2.4 過学習と正則化
- 過学習: 訓練データでは良いが未知データで悪化
- 代表的対策: 早期終了、ドロップアウト、データ拡張、重み減衰（L2）
$$
\mathcal{L} = \mathcal{L}_{\mathrm{data}} + \lambda \|W\|_2^2
$$


## 3. 材料データをNNに入れる「表現」の選び方

材料データでは、まず「何を一定とみなすか（対称性）」が重要です。

### 3.1 組成ベース（構造が不要・速い）
- 入力例: 元素比、平均原子番号、原子半径の統計量など
- 典型モデル: 多層パーセプトロン（MLP）
- 注意: 同一組成で多相がある・結晶構造の差を表現しにくい

### 3.2 構造ベース（結晶構造や局所環境を使う）
- 入力例: 原子座標、近接関係、局所環境記述子
- 典型モデル: グラフニューラルネットワーク（GNN）
- 周期境界条件・格子ベクトル・近接リストを取り扱う

### 3.3 対称性を守る（材料で特に効く）
- 回転・並進で物理量が変わらない（またはテンソルとして変換する）
- その性質をモデル構造に組み込むと、データ効率と汎化が改善しやすい
- 例: $E(3)$ 等変（equivariant）GNN（原子配置の幾何を正しく扱う）


## 4. 代表的アーキテクチャと材料データの対応表

| データの型 | 具体例 | よく使うモデル | ねらい |
|---|---|---|---|
| ベクトル表現 | 組成特徴、統計量 | MLP | まずはベースライン |
| 画像 | 組織像、SEM、磁区、回折像 | CNN / ViT | 空間パターンの学習 |
| スペクトル（1次元系列） | XRD、XAFS、XPS、MBN波形 | 1D-CNN / Transformer | ピーク形状・系列相関 |
| グラフ（結晶） | 原子と結合（近接） | CGCNNなどのGNN | 構造→物性予測 |
| 幾何等変グラフ | 原子配置（座標） | NequIP系など | 力・応力・PES学習 |


## 5. 材料科学での具体例

### 5.1 結晶構造から物性予測（GNN）
- Crystal Graph Convolutional Neural Networks（CGCNN）
  - 結晶を「原子ノード＋近接エッジ」のグラフにして物性予測
  - 局所環境が物性にどう寄与したかを解析する方向にもつなげやすい
  - 参考: PRL版やarXiv版（CGCNN）

### 5.2 原子間ポテンシャル（ニューラルネットワークポテンシャル / MLIP）
- NequIP（E(3)-equivariant GNN）
  - 力・エネルギーを高精度に学習し、MDへ接続しやすい
  - 参考: Nature Communications（NequIP）
- M3GNet（GNNベースの汎用IAP）
  - 幅広い材料空間を狙った「汎用」ポテンシャルの代表例
  - 参考: Nature Computational Science（M3GNet）
- CHGNet（事前学習済みの汎用NNポテンシャル）
  - エネルギー・力・応力などを大規模データで事前学習し、材料一般に使える方向
  - 参考: Nature Machine Intelligence（CHGNet）

### 5.3 産業・プラットフォーム
- Matlantis / PFP
  - 汎用NNポテンシャルを用いた原子レベルシミュレータとして継続的にアップデート情報が公開されている
  - 参考: Matlantisのリリース情報、PFN/ENEOSニュースリリース
- UMA（Universal Models for Atoms）
  - 複数ドメインの大規模データから学習した「汎用モデル」群として公開情報がある
  - 参考: arXiv、Meta AI Researchページ


## 6. 注意点

### 6.1 分割（リーク対策が最重要）
- ランダム分割だけでなく、次も検討する
  - 組成で分割（未知元素比への汎化を見る）
  - 構造型で分割（未知プロトタイプへの汎化を見る）
  - 時系列で分割（実験条件の変化がある場合）

### 6.2 指標（回帰の基本）
- MAE（平均絶対誤差）, RMSE, $R^2$
- 材料探索では「ランキングの良さ」も重要になる（上位選抜の再現率など）

### 6.3 不確かさ（使いどころ）
- 予測値だけでなく不確かさ推定があると実験計画に役立つ
- 例: アンサンブル、MCドロップアウト、分布外検知（OOD）


### 6.4 よくある落とし穴
- データの重複（同一構造・近い構造が train/test をまたぐ）
- 目的変数の定義揺れ（条件違いの混在、温度・欠陥・相の混在）
- 物理的に不自然な外挿（元素置換の外挿、希少元素領域）
- スケールの違い（エネルギー/原子、体積正規化、単位系混在）

## まとめ
ニューラルネットワークは「表現＋学習＋評価」を一体で設計すると材料研究で強力な武器になります。まずは小さなベースライン（組成特徴でMLPなど）で再現できる範囲を確認し、次に構造・対称性を取り込むGNNや等変モデルへ拡張するのが堅実です。