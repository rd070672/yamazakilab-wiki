# CNN（畳み込みニューラルネットワーク）

CNNは、局所近傍に同一のフィルタ（重み）を滑らせて特徴を抽出する「畳み込み」を核とする深層学習アーキテクチャである。材料科学では、顕微鏡像・回折/分光スペクトル・3Dトモグラフィなど、空間（あるいは空間＋周波数）構造を持つデータから、階層的な表現を学習する枠組みとして使われる。

## 参考ドキュメント
- Y. LeCun et al., Gradient-Based Learning Applied to Document Recognition (1998)
  https://yann.lecun.com/exdb/publis/pdf/lecun-01a.pdf
- J. Ling et al., Building Data-driven Models with Microstructural Images (2017)
  https://arxiv.org/abs/1711.00404
- キカガク教材：畳み込みニューラルネットワークの理論
  https://free.kikagaku.ai/tutorial/basic_of_computer_vision/learn/pytorch_image_processing


## 1. 位置づけ
CNNは「局所性」と「重み共有」によって、画像や信号のパターンを効率よく学ぶ設計である。材料分野では、局所欠陥や組織形態の抽出、ピーク形状の学習、ノイズ除去・セグメンテーションなどに強い。

| 観点 | MLP（全結合） | CNN | RNN | Transformer | GNN |
|---|---|---|---|---|---|
| 主な帰納バイアス | なし（汎用） | 局所性・並進等変性・重み共有 | 時系列の逐次性 | 全体参照（注意） | グラフ構造・近傍伝播 |
| 得意な入力 | 固定長ベクトル | 格子状データ（画像、スペクトル） | 系列 | トークン化可能な系列/集合 | 原子/結合・ネットワーク |
| 材料科学で多い用途 | 記述子回帰 | 顕微鏡像、回折/分光、CT | 時系列測定 | 文献・スペクトル統合 | 結晶構造・分子 |

補足として、CNNは「画像だけ」の手法ではなく、1D CNNでスペクトル、3D CNNで体積像（トモグラフィ）にも自然に拡張できる。

## 2. 畳み込み（Convolution）
入力をチャネル付き2次元配列（画像）として $X\in\mathbb{R}^{H\times W\times C_{\mathrm{in}}}$、カーネル（フィルタ）を $W\in\mathbb{R}^{K_h\times K_w\times C_{\mathrm{in}}\times C_{\mathrm{out}}}$ とする。

代表的な2D畳み込み（相互相関として実装されることも多い）は
$$
Y[i,j,k] = \sum_{u=0}^{K_h-1}\sum_{v=0}^{K_w-1}\sum_{c=0}^{C_{\mathrm{in}}-1}
W[u,v,c,k]\;X[i+u,j+v,c] + b[k]
$$
である（$b$ はバイアス）。

この形が重要である理由は次である。
- 局所性：近傍 $K_h\times K_w$ から特徴を作る
- 重み共有：同じ $W$ を全位置で使う
- 並進等変性：入力を平行移動すると出力も同様に平行移動しやすい（画像内の位置に依存しにくい特徴抽出）

### 2.1 ストライド・パディング・出力サイズ
ストライド $S$、パディング $P$ のとき（簡単化のため正方で表記）出力の空間サイズは
$$
H_{\mathrm{out}}=\left\lfloor\frac{H+2P-K}{S}\right\rfloor+1,\qquad
W_{\mathrm{out}}=\left\lfloor\frac{W+2P-K}{S}\right\rfloor+1
$$
となる。測定データを扱う材料分野では「解像度・視野・スケールバー」がタスクに直結するため、どの段階でダウンサンプリングするかが支配的になりやすい。

### 2.2 1D/3D畳み込み
- 1D CNN：$XRD, XAFS, XPS$ のようなスペクトル（エネルギー/角度軸に沿う信号）
- 3D CNN：X線CT・FIB-SEMトモグラフィ・3D EBSDなどの体積データ

## 3. 構成要素（ブロック設計）
CNNは「畳み込み層を積む」だけでなく、学習安定化と表現力のための部品を組み合わせて使う。

- 活性化関数：ReLU など（非線形性の付与）
- 正規化：BatchNorm など（学習の安定化）
- プーリング/ストライド畳み込み：解像度を落として受容野を広げる
- 残差接続（Residual）：深層化しても最適化しやすくする
- グローバル平均プーリング：最終段で空間平均し、パラメータを減らす

材料画像ではセグメンテーション（相/析出物/欠陥領域の切り出し）が多いため、分類用CNNよりも「ピクセルごとの予測」を行う構造が頻出である。

## 4. 代表的アーキテクチャとタスク対応
- 分類（組織型、相、欠陥の有無）
  - 流れ：Backbone（特徴抽出CNN）→ 全体特徴 → クラス確率
- 回帰（物性推定、特徴量の連続推定）
  - 流れ：Backbone → プーリング → 回帰ヘッド
- セグメンテーション（相領域、粒界、析出物、ポア）
  - 例：U-Net系（縮小経路＋拡大経路＋スキップ接続）

## 5. 入力設計：何をCNNに入れるか
材料データは「見た目が似ていても条件が違う」「装置差が大きい」という難しさがあるため、入力の定義と正規化が実務上の勝負になる。

| データ | 代表入力 | よくあるタスク | CNNの型 | 注意点 |
|---|---|---|---|---|
| SEM/TEM/STEM像 | 2D画像（多チャネル可） | 組織分類、欠陥検出、ノイズ除去 | 2D CNN / U-Net | 倍率・コントラスト差、スケール統一 |
| EBSD | 方位/相/CI等をチャネル化 | 粒界抽出、組織推定 | 2D CNN / U-Net | 角度表現（周期性）と前処理 |
| XRD/XAFS/XPS | 1D信号（場合により多チャネル） | 相推定、ピーク属性推定 | 1D CNN | 軸の校正、バックグラウンド、分解能差 |
| X線CT/トモグラフィ | 3Dボクセル | ボイド/析出物の抽出 | 3D CNN / 3D U-Net | アノテーションコスト、メモリ制約 |

## 6. 学習・評価の要点
### 6.1 データ分割（リーク対策）
材料では、同一試料の切り出しパッチが訓練と評価に混ざると、見かけ性能が上がりやすい。試料ID、ロット、熱処理条件、測定日など「生成過程単位」で分割する設計が重要である。

### 6.2 ドメインシフト（装置差・条件差）
同じ組織でも、加速電圧・検出器・前処理で見え方が変わる。現場では
- 正規化（ヒストグラム、強度スケーリング）
- データ拡張（ノイズ、ぼけ、回転、スケール変換）
- 転移学習（外部データで事前学習→少数ラベルで微調整）
が実効的である。

### 6.3 ラベルが少ない場合
- パッチ学習（大画像をタイル化し弱教師で学習）
- 自己教師あり（回転予測、対比学習など）で特徴抽出器を作り、少数データで下流学習
が有効になりやすい。

## まとめ
CNNは、局所性と重み共有により、画像・スペクトル・体積像といった「構造を持つ材料データ」から階層的特徴を学習するアーキテクチャである。材料科学では、入力のスケール整合、装置差への頑健化、リークを避けた分割設計が性能と信頼性を支配し、目的に応じて1D/2D/3D CNNやU-Net系を選び分けるのが実務的である。
