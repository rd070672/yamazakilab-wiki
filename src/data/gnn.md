# グラフニューラルネットワーク（GNN: Graph Neural Network）

グラフニューラルネットワーク（GNN）は、ノード（点）とエッジ（辺）で表現されるデータ上で、関係性を保った表現学習と予測を行う深層学習アーキテクチャである。分子・結晶・欠陥・界面など「相互作用が本質」の対象を、同一の枠組みで扱える点が特徴である。

## 参考ドキュメント
- Gilmer et al., Neural Message Passing for Quantum Chemistry (MPNN枠組み)
  https://arxiv.org/abs/1704.01212
- Xie and Grossman, Crystal Graph Convolutional Neural Networks (結晶グラフでの物性予測)
  https://link.aps.org/doi/10.1103/PhysRevLett.120.145301
- Qiita: グラフニューラルネットワーク（GNN）に入門する（日本語）
  https://qiita.com/ymgc3/items/a809d98abde5251bea15


## 1. 位置づけ：NN / CNN / RNN / GNN の比較
深層学習における違いは、主に入力表現と帰納バイアス（どの構造を前提に学ぶか）で決まる。

| アーキテクチャ | 入力の典型 | 帰納バイアス（強い前提） | 得意な依存関係 |
|---|---|---|---|
| MLP（通常のNN） | 固定長ベクトル | 特になし（全結合） | 特徴間の一般的相互作用 |
| CNN | 格子（画像、2D/3Dボクセル） | 局所性、平行移動に対する同型性 | 近傍のパターン、局所構造 |
| RNN/LSTM/GRU | 系列（時系列、文章） | 順序、逐次更新 | 時間方向・順序依存 |
| GNN | グラフ（ノード＋エッジ） | 関係性、置換不変（ノード順序に依存しない） | ネットワーク構造の相互作用（近傍→多段で遠方） |

要点は、CNNが「格子上の近傍」、RNNが「順序付きの近傍（過去→現在）」を中心に学ぶのに対し、GNNは「任意の接続関係（トポロジー）に沿った近傍」を中心に学ぶ設計である。

## 2. グラフ表現
材料データは関係（relational）として表現しやすい。

- 分子：原子＝ノード、結合＝エッジ
- 結晶：原子＝ノード、近接（距離カットオフ）＝エッジ、周期境界（PBC）を含む
- 欠陥・界面・アモルファス：局所環境や結合ネットワークの差が本質になりやすい

結晶をグラフで扱い物性を予測する代表例として、CGCNNが提案されている。

## 3. 中核の数式：メッセージパッシング（MPNNの一般形）
材料向けGNNの多くはメッセージパッシングとして統一的に理解できる。

グラフを $G=(V,E)$ とし、ノード $v$ の層 $k$ における表現を $h_v^{(k)}$、エッジ特徴を $e_{uv}$ とする。

1) メッセージ生成
$$
m_{uv}^{(k)} = \psi\!\left(h_u^{(k)}, h_v^{(k)}, e_{uv}\right)
$$

2) 近傍集約（順序に依存しない集約を用いる）
$$
M_v^{(k)} = \square_{u\in\mathcal{N}(v)} m_{uv}^{(k)}
$$
ここで $\square$ は sum / mean / max などである。

3) 更新
$$
h_v^{(k+1)} = \phi\!\left(h_v^{(k)}, M_v^{(k)}\right)
$$

グラフ全体の予測（物性回帰など）は readout でまとめることが多い。
$$
h_G = \rho\!\left(\sum_{v\in V} h_v^{(K)}\right),\quad \hat{y}=g(h_G)
$$

sum/mean による readout は、ノードの並べ替え（置換）に対して不変になりやすく、原子番号の並び順に依存しない設計になりやすい。

## 4. CNN・RNNとの関係
### 4.1 CNNとの関係：畳み込みの一般化
CNNは格子上で「固定された近傍（例：3×3）」に畳み込みカーネルを適用する設計である。
GNNは「近傍の集合 $\mathcal{N}(v)$」がデータごとに変わる状況で、同様の“近傍混合”をメッセージパッシングとして実装する設計である。

直感的には、CNNの畳み込み
$$
h_{i}^{(k+1)} = \sigma\!\left(\sum_{j\in\mathcal{N}(i)} W\,h_j^{(k)}\right)
$$
を、グラフの近傍定義へ拡張したものがGNNの基本形に近い。

GCNはこの方向を明確にした代表例である。

### 4.2 RNNとの関係：近傍集約の反復＝再帰更新
RNNは時間方向に状態を更新するが、GNNは「グラフ距離（hop）」方向に状態を更新する。
- 1層で 1-hop 近傍を混合
- K層で K-hop 先まで情報が伝播

したがって、RNNが「系列の文脈」を作るのに対し、GNNは「ネットワーク上の文脈」を作ると整理できる。

## 5. 代表的派生（選び方の軸）
### 5.1 近傍に重みを付ける：GAT（Attention）
近傍ごとに重要度を学習して重み付き集約をする設計である。
例として、注意係数 $\alpha_{uv}$ を
$$
\alpha_{uv}=\mathrm{softmax}_u\big(a(h_u,h_v)\big)
$$
として
$$
h_v^{(k+1)}=\sigma\!\left(\sum_{u\in\mathcal{N}(v)}\alpha_{uv}W h_u^{(k)}\right)
$$
のように更新する（代表形）。

### 5.2 大規模・未知ノードへ：GraphSAGE（Inductive）
固定グラフ前提ではなく、見たことのないノード・グラフへ一般化しやすい枠組みである。

### 5.3 材料の幾何学（距離・回転）：幾何GNN／等変性
原子座標を用いる場合、回転・並進の対称性を壊さない設計が鍵になる。
- SchNetは連続フィルタ畳み込みで座標情報を扱う設計を与える。
- NequIPは $E(3)$ 等変性を明示的に組み込み、データ効率の良いポテンシャル学習を狙う。
これらは「結晶・分子の幾何学的対称性」をモデル側に組み込む流れとして、幾何深層学習（Geometric Deep Learning）の文脈で整理される。

## 6. 材料科学での入力設計
- ノード（原子）特徴：元素埋め込み、（必要なら）電気陰性度や原子半径など
- エッジ特徴：距離 $r$、（必要なら）角度・多体情報
- 近接定義：距離カットオフ $r_c$ とPBCの扱い
- 目的変数の単位・条件：温度、相、磁気状態、計算設定を揃える

結晶グラフを直接入力として物性を予測する代表例としてCGCNNや、分子・結晶を統一的に扱うMEGNetなどが提示されている。:contentReference[oaicite:9]{index=9}

## 7. 典型タスク
- グラフ（結晶）回帰：形成エネルギー、弾性、バンドギャップ、磁気量など
- ノード回帰：サイトごとの電荷、局所磁気モーメント、局所環境指標
- エッジ回帰：結合の性質、相互作用の推定（モデル設計による）

## 8. 限界と設計論点
- 受容野の限界：K層でK-hopまでしか直接混合しない（長距離相互作用は工夫が要る）
- oversmoothing：層を深くすると表現が均一化して識別性が落ちる問題が知られる（残差、正規化、設計の工夫が必要になる）
- グラフ構築依存：カットオフや特徴設計が結果を支配しやすい
- データ分割の難しさ：材料は近縁系が多く、分割次第で見かけ性能が変わりやすい

## まとめ
GNNは、関係（グラフ）を前提とした「近傍集約＋更新」を繰り返すことで表現を学習する深層学習アーキテクチャである。CNNが格子、RNNが順序を前提にするのに対し、GNNは任意の接続関係を前提にできるため、分子・結晶・欠陥など材料系の構造データと相性が良い。一方で、グラフ構築（近接定義、PBC、特徴量）と対称性（回転・並進）の扱いが性能を支配しやすく、用途に応じてGAT/GraphSAGE/幾何GNN・等変性モデルを選ぶ設計が要点である。
