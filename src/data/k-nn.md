# k近傍法（k-Nearest Neighbors, kNN）

k近傍法は、入力と訓練データの類似度（距離）にもとづき、近いデータ（近傍）のラベルや値から予測を行う教師あり学習である。材料科学では、組成・構造記述子・プロセス条件などの表形式データに対して、強いベースラインと類似材検索の中核として機能する手法である。

## 参考ドキュメント
- Cover, T. and Hart, P., Nearest Neighbor Pattern Classification (1967)
  https://isl.stanford.edu/~cover/papers/transIT/0021cove.pdf
- scikit-learn User Guide: Nearest Neighbors
  https://scikit-learn.org/stable/modules/neighbors.html
- IBM: k近傍法（kNN）アルゴリズム（日本語）
  https://www.ibm.com/jp-ja/think/topics/knn


## 1. 位置づけ
- 学習パラダイム：教師あり学習（分類・回帰）
- モデル特性：非パラメトリック、インスタンスベース（怠惰学習）である
- 学習の直観：似ている材料は似た物性・似た状態を示す、という仮定に依存する

材料データでは、物理的に妥当な類似度を設計できるほど性能が伸びやすい。

## 2. アルゴリズムの基本
入力特徴量を $x$、訓練集合を ${(x_i, y_i)}$ とし、距離関数を $d(x, x_i)$ とする。

1. すべての訓練点について距離 $d(x, x_i)$ を計算する
2. 距離が小さい順に k 個の近傍集合 $N_k(x)$ を選ぶ
3. 近傍の情報から分類または回帰を行う

### 2.1 分類（多数決・重み付き多数決）
クラス集合を $C$ とし、重み $w_i$ を与えるとき、予測は次で与えられる。
$$
\hat{y}(x) = \arg\max_{c\in C}\sum_{i\in N_k(x)} w_i \,\mathbb{I}(y_i=c)
$$
- 一様重み：$w_i = 1$
- 距離重み：$w_i = 1/(d(x,x_i)+\epsilon)$ など（近い材料を重視する）

### 2.2 回帰（平均・重み付き平均）
$$
\hat{y}(x)=\frac{\sum_{i\in N_k(x)} w_i y_i}{\sum_{i\in N_k(x)} w_i}
$$
材料物性の局所補間として解釈できるため、近傍分散を不確かさ指標として併用できる場合がある。

## 3. 距離（類似度）の設計が支配的である
### 3.1 代表的な距離
Minkowski距離（ $p$ ノルム）は次である。
$$
d_p(x,x')=\left(\sum_j |x_j-x'_j|^p\right)^{1/p}
$$
- $p=2$：ユークリッド距離
- $p=1$：マンハッタン距離

高次元やスパース特徴では、コサイン距離などが有利なこともある。

### 3.2 距離設計
- 組成：元素比＋元素物性統計（平均・分散など）を結合し距離を定義する
- 結晶構造：局所環境記述子の距離を使う（記述子選択が要である）
- スペクトル：ピーク列（位置・強度・幅）やパッチ特徴に落とし距離を定義する
- 画像：形態量・テクスチャ特徴に落とし距離を定義する
- 埋め込み：自己教師ありやTransformer/GNNで得た埋め込み空間で距離を測る

どの距離を採るかは、何を同一視するか（同一相、同一機構、同一合成経路など）の仮説そのものである。

## 4. 計算量と実装上の論点
- 学習コスト：基本的に訓練データを保持するだけである
- 推論コスト：単純実装では、各クエリで全点との距離計算が必要になりやすい

そのため、近傍探索の高速化として次が用いられる。
- KD-tree、Ball tree などの木構造（低次元で有利なことが多い）
- 近似近傍探索（ANN：大規模埋め込み検索で有用）

材料探索で候補数が多い場合、近傍探索の実装選定が実務上のボトルネックになりやすい。

## 5. kの選び方と過学習・汎化
- $k$が小さい：局所に過剰適合しやすい（ノイズに弱い）
- $k$が大きい：平均化が強くなり、境界がぼやけやすい（バイアス増）

標準的には、交差検証でkを選ぶのが基本である。二値分類では偶数kによる同票を避ける設計も現実的である。

## 6. 重要な前処理
### 6.1 スケーリング
距離ベースであるため、特徴量の単位・スケール混在に弱い。物性・プロセス条件・統計記述子を混ぜる場合、スケーリング方針を固定することが重要である。

### 6.2 欠損値への対応
表形式の材料データは欠損が多い。kNNは、近傍平均で欠損を補完する枠組み（kNN imputation）としても活用される。
ただし、欠損パターン自体が情報である場合や、外挿領域では補完が破綻しやすい点に注意が必要である。

### 6.3 次元の呪い
高次元では距離の差がつきにくくなり、近傍概念が弱くなる。対策は次の通りである。
- 特徴選択（物理的に意味のある特徴へ寄せる）
- 次元削減（PCA/UMAPなどで埋め込みを作り、その空間で近傍を取る）
- 距離関数の変更（コサインなど）や表現学習の導入

## 7. 評価設計
- 近縁材料リークに注意が必要である（同一系列の組成掃引や類似構造が分割を跨ぐと過大評価になりやすい）
- 外挿評価が重要である（未知元素系、未知組成域、未知プロセス域）
- kNNは類似度依存であるため、訓練分布から遠い点では急激に性能が落ちやすい

## 8. 使い分けの目安（他手法との比較）
- kNN：少量データのベースライン、類似材検索、局所補間、欠損補完に強い
- SVM：マージン最大化で高次元でも頑健だが、カーネル選択とスケールに敏感である
- 勾配ブースティング：表形式で高精度になりやすいが、外挿の挙動とリークに注意が必要である

材料分野では、まずkNNで類似空間が妥当かを点検し、その後にGBDTや深層表現学習へ移る流れが実装しやすい。

## まとめ
k近傍法は、類似度にもとづく近傍の多数決・平均で予測する非パラメトリックな教師あり学習であり、材料科学ではベースラインと類似材検索の両面で有用である。実用上の鍵は、距離（表現）設計、スケーリング、リークを避けた分割、外挿評価の設計にある。

