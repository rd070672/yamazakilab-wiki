# パターン認識と機械学習
＋

本書は、パターン認識と機械学習を「確率モデル」と「ベイズ的推論」を中核にして統一的に説明する教科書である。回帰・分類・密度推定・次元削減・時系列・モデル結合までを、同じ数理語彙で横断することを目的とする。

## 参考ドキュメント
1. Christopher M. Bishop, Pattern Recognition and Machine Learning, Springer（書誌情報ページ）
   https://link.springer.com/book/9780387310732
2. Pattern Recognition and Machine Learning（PDF, Microsoft Research）
   https://www.microsoft.com/en-us/research/wp-content/uploads/2006/01/Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf
3. awesome-prml-ja（日本語の読書会・資料リンク集）
   https://github.com/tsg-ut/awesome-prml-ja

## 1. 本書の位置づけ
PRML（Pattern Recognition and Machine Learning, Bishop 2006）は、機械学習を「最適化テクニックの寄せ集め」としてではなく、確率分布としてのモデル化と、その上での推論・意思決定として再構成する本である。  
データサイエンスの観点では、以下が本書の中心的価値である。

- 観測データを生成する仕組みを確率変数の関係として表し、学習を確率分布の推定として扱う枠組みを提供する。
- 予測は点推定だけでなく、不確かさ（予測分布）として扱う立場を強調する。
- 回帰・分類・クラスタリング・潜在変数・グラフィカルモデル・近似推論を、同一の基本構造（尤度・事前分布・事後分布・周辺化）で結び付ける。

## 2. 基本方程式
### 2.1 データとモデル
入力ベクトル $x \in \mathbb{R}^{D}$、目的変数 $t$（回帰なら実数、分類なら離散ラベル）を観測する。データ集合を
$$
\mathcal{D}=\{(x_n,t_n)\}_{n=1}^{N}
$$
とする。モデルはパラメータ $w$（重み）を持ち、条件付き分布 $p(t\mid x,w)$（識別モデル）や同時分布 $p(x,t\mid w)$（生成モデル）として表される。

### 2.2 学習（推定）
- 最尤推定（MLE）  
  $$
  w_{\mathrm{ML}}=\arg\max_w \; p(\mathcal{D}\mid w)
  $$
- 最大事後確率推定（MAP）  
  $$
  w_{\mathrm{MAP}}=\arg\max_w \; p(w\mid\mathcal{D})
  =\arg\max_w \; p(\mathcal{D}\mid w)p(w)
  $$
- ベイズ推論（パラメータも確率変数）  
  $$
  p(w\mid\mathcal{D})=\frac{p(\mathcal{D}\mid w)p(w)}{p(\mathcal{D})}
  $$
  予測は
  $$
  p(t_\ast\mid x_\ast,\mathcal{D})=\int p(t_\ast\mid x_\ast,w)\,p(w\mid\mathcal{D})\,dw
  $$
  で与える。

### 2.3 意思決定（decision）
誤分類率最小化や期待損失最小化は、事後確率に基づく規則として表される。損失関数 $L(t,\hat{t})$ に対し、
$$
\hat{t}(x)=\arg\min_{\hat{t}}\;\mathbb{E}[L(t,\hat{t})\mid x]
$$
が基本となる。

### 2.4 情報理論（KL, 交差エントロピー）
分布間の距離として相対エントロピー（KL）が頻出する。
$$
\mathrm{KL}(q\|p)=\int q(z)\log\frac{q(z)}{p(z)}\,dz
$$
変分推論やモデル比較で、下界（ELBO）として現れる。

## 3. 主要テーマの俯瞰
PRMLの構造は大きく次の連鎖で理解しやすい。

- 基礎：確率分布・推定・決定・情報量（1–2章）
- 線形モデル：回帰と分類を統一して扱う（3–4章）
- 非線形化：ニューラルネット・カーネル法（5–6章）
- スパース性：SVM・RVM（7章）
- 構造表現：グラフィカルモデル（8章）
- 潜在変数：混合モデルとEM（9章）
- 近似推論：変分・EP（10章）
- サンプリング：MCMC・HMC（11章）
- 次元削減：PCAから非線形潜在変数へ（12章）
- 時系列：HMM・状態空間モデル（13章）
- モデル結合：アンサンブル・ブースティング等（14章）

## 4. 各論

### 第1章 Introduction
本書の用語と目的を、曲線フィッティングの例で導入する章である。頻出概念（過学習、正則化、ベイズ、決定理論、情報理論）を最初にまとめる。

- 多項式回帰の例：基底 $\phi(x)$ と線形モデルの形
  $$
  y(x,w)=\sum_{j=0}^{M} w_j x^j = w^{\mathsf{T}}\phi(x)
  $$
- 二乗誤差最小化と正則化（リッジ）
  $$
  E(w)=\frac{1}{2}\sum_{n}(y(x_n,w)-t_n)^2 + \frac{\lambda}{2}\|w\|^2
  $$
- 次元の呪い：$D$ が増えると必要データ量や密度推定の難度が急増するという直観を与える。
- 決定理論：事後確率と損失最小化の関係（分類と回帰の共通視点）。
- 情報理論：エントロピー、相互情報量、KLの基礎。

### 第2章 Probability Distributions
確率分布のカタログではなく、「分布を構成して推論に使う」ための道具立てを整理する章である。

- 離散：ベルヌーイ、二項、カテゴリカル・多項、共役事前分布としてのベータ、ディリクレ。
- 連続：ガウス分布、条件付きガウス、周辺化と条件付け、ブロック行列による扱い。
- 指数型分布族と共役事前分布：
  $$
  p(x\mid\eta)=h(x)\exp(\eta^{\mathsf{T}}u(x)-A(\eta))
  $$
  を基軸に、学習・推論が整理される。
- 非ガウス：t分布、フォン・ミーゼスなど、ロバスト性や周期性の観点で登場する。
- 変数変換とヤコビアン、デルタ関数の扱いなど、後半章で必要な基礎も含む。

### 第3章 Linear Models for Regression
回帰問題を、確率モデル化して推定する章である。線形回帰を「尤度＋事前分布＋事後分布」で捉え直す点が核である。

- ガウス雑音モデル：
  $$
  p(t\mid x,w,\beta)=\mathcal{N}(t\mid w^{\mathsf{T}}\phi(x),\beta^{-1})
  $$
- 最尤解：正規方程式、行列での閉形式解。
- ベイズ線形回帰：事前 $p(w)=\mathcal{N}(w\mid 0,\alpha^{-1}I)$ により、事後もガウスになる。
- 予測分布：平均だけでなく分散（不確かさ）を含む。
- リッジ回帰とガウス事前の同値性、証拠近似（evidence approximation）への接続。

### 第4章 Linear Models for Classification
分類を確率モデルとして定式化し、線形判別関数の下で多様なモデルが統一されることを示す章である。

- ロジスティック回帰（二値）：
  $$
  p(t=1\mid x)=\sigma(w^{\mathsf{T}}\phi(x)),\quad \sigma(a)=\frac{1}{1+e^{-a}}
  $$
- 交差エントロピー損失（負の対数尤度）と勾配・ヘッセ行列。
- Newton法・IRLS（反復再重み付け最小二乗）としての見通し。
- 多クラス：softmax
  $$
  p(t=k\mid x)=\frac{\exp(a_k)}{\sum_j \exp(a_j)},\quad a_k=w_k^{\mathsf{T}}\phi(x)
  $$
- 生成モデルによる分類：クラス条件付き $p(x\mid C_k)$ とベイズ則で $p(C_k\mid x)$ を得る視点も併置する。

### 第5章 Neural Networks
ニューラルネットを「非線形基底関数を学習するモデル」として扱い、最適化・正則化・ベイズ化までを含む章である。

- 多層パーセプトロン（MLP）を関数近似として導入。
- 誤差逆伝播法、勾配降下、収束性と学習率。
- 出力層と確率モデル：回帰（ガウス）、分類（softmax）を整合させる。
- 正則化：重み減衰、早期終了、データ拡張的な不変性、接線伝播。
- 2階情報：ヘッセ行列、近似、有限差分。
- 混合密度ネットワーク：$p(t\mid x)$ をガウス混合などで表現する発想。
- ベイズニューラルネット：重みに事前分布を置き、予測分布を得る枠組み。

### 第6章 Kernel Methods
特徴写像 $\phi(x)$ を明示せず、内積 $k(x,x')=\phi(x)^{\mathsf{T}}\phi(x')$ で表現するための章である。

- 双対表現：解がデータ点の線形結合として書けること。
- カーネルの構成：正定値性（Mercer条件）と代表的カーネル。
- RBFネットワーク：基底中心と幅の扱い。
- ガウス過程（GP）：関数 $f(x)$ 自体を確率過程として置く。
  - 回帰：事前共分散 $K$ による予測平均・分散
  - ハイパーパラメータ学習：周辺尤度最大化
  - ARD：次元ごとの関連度を推定する考え方
  - 分類：ラプラス近似などの近似推論へ接続

### 第7章 Sparse Kernel Machines
カーネル法の大規模化・解釈性の観点で重要な「スパースな解」を扱う章である。

- 最大マージン分類器としてのSVM：制約付き最適化と双対問題。
- 多クラスSVM、回帰SVM、計算学習理論との接続。
- Relevance Vector Machine（RVM）：ベイズ的スパース化（自動関連度決定）により、少数の関連ベクトルで表現する。
- スパース性の解析：事前分布と事後の収縮、過学習抑制の理解。

### 第8章 Graphical Models
確率変数間の依存構造をグラフで表し、推論を計算として扱う章である。

- ベイジアンネットワーク（有向）とマルコフ確率場（無向）。
- 条件付き独立性、D-separation。
- 因子分解と局所計算：確率の積で表して周辺化する。
- 推論アルゴリズム：
  - 連鎖・木構造での厳密推論
  - 因子グラフ、sum-product、max-sum
  - ループありの近似（loopy belief propagation）
- 構造学習：モデル選択と結び付く。

### 第9章 Mixture Models and EM
潜在変数モデルの基本として、混合モデルとEMアルゴリズムを扱う章である。

- $K$-means：最小化問題としてのクラスタリング。
- ガウス混合モデル（GMM）：
  $$
  p(x)=\sum_{k=1}^{K}\pi_k \mathcal{N}(x\mid \mu_k,\Sigma_k)
  $$
- EMアルゴリズム：
  - Eステップ：責務 $r_{nk}=p(z_k=1\mid x_n)$
  - Mステップ：期待完全データ対数尤度の最大化
- EMの一般形：下界最大化としての見方（10章の変分推論へつながる）。
- ベルヌーイ混合、ベイズ線形回帰へのEM適用など、展開例を含む。

### 第10章 Approximate Inference
厳密推論が困難なときの近似推論を体系化する章である。変分推論と期待伝播（EP）が中心となる。

- 変分推論：$p(z\mid x)$ を $q(z)$ で近似し、KL最小化で導く。
  - 因子化分布 $q(z)=\prod_i q_i(z_i)$
  - ELBO（下界）最大化
- 変分混合ガウス、変分線形回帰、指数型分布族と変分メッセージパッシング。
- 局所変分法：ロジスティック回帰などでの補助変数導入。
- 期待伝播（EP）：局所近似を繰り返し、モーメント一致で全体を整合させる。疎な解釈や予測性能の観点で意義がある。

### 第11章 Sampling Methods
近似推論の別系統として、サンプリングで周辺化や期待値計算を行う章である。

- 基本サンプリング：棄却法、重点サンプリング、SIR（resampling）。
- MCMC：定常分布を目的分布にするマルコフ連鎖。
  - Metropolis-Hastings
  - Gibbs sampling
  - Slice sampling
- Hybrid Monte Carlo（HMC）：勾配情報を使い、高次元での混合性を改善する考え方。
- 分配関数（正規化定数）の推定など、エネルギーベースモデルに繋がる基礎。

### 第12章 Continuous Latent Variables
連続潜在変数モデルとして、次元削減の中心概念を体系化する章である。

- PCA：最大分散問題、最小再構成誤差としての定式化。
- 確率的PCA：線形ガウス潜在変数モデルとしてのPCA。
  $$
  x = W z + \mu + \epsilon,\quad z\sim\mathcal{N}(0,I),\;\epsilon\sim\mathcal{N}(0,\sigma^2 I)
  $$
- 因子分析：雑音共分散の扱いの違い。
- カーネルPCA：非線形構造をカーネルで扱う。
- 非線形潜在変数モデル：独立成分分析（ICA）、自己連想ネット、非線形多様体のモデル化へ展開する。

### 第13章 Sequential Data
系列データを確率モデルで扱う章である。状態遷移という構造が中心となる。

- マルコフモデル：状態の遷移確率で系列を表す。
- HMM（隠れマルコフモデル）：
  - forward-backward による周辺化
  - Viterbi による最尤系列推定
  - スケーリング因子で数値安定化
  - 拡張（状態数の増加、出力分布の複雑化など）
- 線形動的システム（LDS）：連続状態の状態空間モデル、カルマンフィルタに接続する。
- 粒子フィルタ：非線形・非ガウスへの拡張。

### 第14章 Combining Models
複数モデルの統合により、精度や安定性を高める考え方を整理する章である。

- ベイズモデル平均（BMA）：モデル不確かさを周辺化する。
- committees：複数予測器の平均化（重み付き等）。
- boosting：逐次的に弱学習器を重ね、指数損失最小化などで理解する。
- 木モデル：分割統治とアンサンブル（後続の発展にも接続しやすい）。
- 条件付き混合モデル：mixture of experts など、入力に応じてモデルを切り替える発想。

## 5. 概念対比の表

### 5.1 タスク別に見たモデル対応
| タスク | 出力の形 | 代表モデル | 関連章 |
|---|---|---|---|
| 回帰 | 連続値 | 線形回帰、ベイズ線形回帰、GP回帰 | 3, 6 |
| 二値分類 | 確率（0/1） | ロジスティック回帰、SVM、GP分類 | 4, 7, 6 |
| 多クラス分類 | 確率ベクトル | softmax回帰、多クラスSVM | 4, 7 |
| 密度推定 | 分布 $p(x)$ | GMM、混合ベルヌーイ、潜在変数モデル | 2, 9 |
| クラスタリング | 潜在ラベル | $K$-means、GMM+EM | 9 |
| 次元削減 | 潜在変数 $z$ | PCA、確率PCA、因子分析、カーネルPCA | 12 |
| 系列推定 | 状態系列 | HMM、LDS、粒子フィルタ | 13 |
| モデル統合 | 予測の合成 | BMA、boosting、mixture of experts | 14 |

### 5.2 推論法の位置づけ
| 推論の考え方 | 代表手法 | 何を近似するか | 関連章 |
|---|---|---|---|
| 厳密推論 | 木上のsum-product等 | 周辺化を正確に計算 | 8 |
| 局所近似 | ラプラス近似 | 事後をガウスで近似 | 6, 10 |
| 下界最大化 | 変分推論（ELBO） | $p(z\mid x)$ を因子化分布で近似 | 10 |
| モーメント整合 | 期待伝播（EP） | 局所因子近似を更新し全体整合 | 10 |
| サンプリング | MCMC, HMC | 期待値や周辺分布をサンプルで近似 | 11 |

## まとめ
PRMLは、機械学習を確率モデリングと推論として統一的に理解するための体系書である。章ごとの技法は多岐に見えるが、尤度・事前分布・事後分布・周辺化という基本骨格で連結されており、回帰から時系列までを同じ視点で整理できる点が本書の中核である。

## 関連研究
- PRML PDF（別ホスト、目次・付録確認に使用）
  https://tjzhifei.github.io/links/PRML.pdf
- PRML Exercises Solutions（web edition）
  https://www.microsoft.com/en-us/research/wp-content/uploads/2016/05/prml-web-sol-2009-09-08.pdf
- 日本語版（章構成の確認に使用）
  https://www.kinokuniya.co.jp/f/dsg-01-9784431100317
- PRML読み始めの参考リンク集（日本語記事）
  https://shuyo.hatenablog.com/entry/20130725/prml
