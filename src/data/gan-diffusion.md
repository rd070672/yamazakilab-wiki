# 生成モデル（GAN, Diffusion models）

生成モデルは、観測データと同じ分布から新しいサンプルを生成する枠組みであり、材料科学では「新規組成・新規構造・新規組織」を探索するための確率的設計器として使われる。GANと拡散モデルは代表的な系列であり、前者は敵対的学習、後者はノイズ除去過程の学習を中核に持つ。

## 参考ドキュメント
- I. Goodfellow et al., Generative Adversarial Nets (NeurIPS 2014)
  https://arxiv.org/abs/1406.2661
- J. Ho et al., Denoising Diffusion Probabilistic Models (NeurIPS 2020)
  https://arxiv.org/abs/2006.11239
- MI-6 技術ブログ：材料開発における夢の技術、結晶構造予測の現在地（MatterGenを含む日本語解説）
  https://milab.mi-6.co.jp/article/e0005


## 1. 位置づけ（予測から生成へ）
材料インフォマティクスの典型は物性予測（回帰・分類）であるが、逆設計では「目的条件を満たす入力（材料）を出す」必要がある。
生成モデルは、次の2つを同時に扱う枠組みである。

- 分布学習：データ分布 p_data(x) を近似して、それらしい材料サンプル x を出す
- 条件付き生成：所望物性 y や条件 c を与えて p(x|c) から生成する（逆設計）

ここで x は材料表現であり、例として次がある。
- 組織画像、3Dトモグラフィ（格子状データ）
- スペクトル（1D信号）
- 原子種・座標・格子（周期境界を含む構造表現）
- グラフ（原子をノード、結合/近接をエッジ）

## 2. GAN（Generative Adversarial Network）
### 2.1 基本アイデア
GANは、生成器 G と識別器 D を競争させることで、Gが本物らしいサンプルを出力するように学習する枠組みである。

- 生成器：ノイズ z からサンプル x̃ = G(z) を生成する
- 識別器：入力が実データか生成データかを判別し、D(x) を出す

### 2.2 代表的な目的関数（ミニマックス）
標準GANの代表形は次である。
$$
\min_G \max_D \;
\mathbb{E}_{x\sim p_{\mathrm{data}}}\left[\log D(x)\right]
+
\mathbb{E}_{z\sim p(z)}\left[\log(1-D(G(z)))\right]
$$
学習は、Dを強くし過ぎるとGへ勾配が流れにくい等の不安定性を持つため、実務では損失の改良や正則化が重要である。

### 2.3 条件付きGAN（cGAN）
材料設計で有用なのは条件付き生成である。条件 c（例えば組成、熱処理条件、目標物性）を入力に加えて
$$
x̃ = G(z, c),\quad D(x, c)
$$
とし、p(x|c) を学習する形式が多い。

### 2.4 強みと弱み
強み
- 生成が高速である（学習後は1回の順伝播でサンプルが得られる）
- 画像系（組織像、相分布、欠陥分布）で高い表現力を持ちやすい
- 条件付き生成（cGAN）でプロセス→組織の写像を作りやすい

弱み
- 学習が不安定になりやすい
- 多様性が失われるモード崩壊（mode collapse）が起きやすい
- 物理制約を入れないと、見た目は良いが物理的に不適切なサンプルが出やすい

## 3. 拡散モデル（Diffusion models）
### 3.1 基本アイデア
拡散モデルは、データに段階的にノイズを加える過程（正方向）と、ノイズからデータへ戻す過程（逆方向）を学習する生成モデルである。
直観的には、ノイズから出発して少しずつ「もっともらしい構造」に近づける反復生成である。

### 3.2 離散時間DDPMの代表式
正方向（ノイズ付加）は、t=1..T に対して
$$
q(x_t|x_{t-1}) = \mathcal{N}\left(\sqrt{1-\beta_t}\,x_{t-1},\; \beta_t I\right)
$$
で定義されることが多い。

学習では、各ステップで加えたノイズ ε を当てる（あるいはデノイズ後の平均を当てる）形に落とし込むのが典型である。
$$
\mathcal{L} = \mathbb{E}_{t, x_0, \varepsilon}\left[\|\varepsilon - \varepsilon_\theta(x_t, t)\|^2\right]
$$
ここで x_0 は実データ、x_t はノイズ付加後の状態、ε_θ はニューラルネットワークである。

### 3.3 連続時間（スコアベース）の見方
拡散モデルはスコア（勾配）学習として再解釈でき、連続時間の確率微分方程式（SDE）を通じて、より一般化された生成枠組みとして扱える。
材料側の重要点は、座標・格子・元素種など多成分の表現に対しても、連続的な更新則を設計しやすい点にある。

### 3.4 強みと弱み
強み
- 学習が比較的安定しやすい
- 多様性を保ちやすく、モード崩壊が起きにくい
- 条件付き生成や誘導（guidance）により、逆設計へ接続しやすい

弱み
- 生成に反復ステップが必要で、サンプリングが重くなりやすい
- 材料表現が高次元かつ制約付きの場合、表現設計と制約処理がボトルネックになる

## 4. 表現設計（何を生成するか）
生成モデルの難所はモデルより表現である。代表例を整理する。

- 組織・相分布（画像、3Dボクセル）
  - 画像GAN、画像拡散モデルがそのまま適用できる
  - 体積分率、相関関数、連結性など統計量制約を入れると実用性が上がる

- スペクトル（1D信号：XRD, XAFS, XPSなど）
  - 1D CNNベースのGAN/拡散で生成できる
  - 軸校正、ピーク位置の物理拘束、バックグラウンド処理が重要である

- 結晶構造（原子種・座標・格子、PBC）
  - 並進・回転・原子入れ替え不変性、周期境界（PBC）を保つ設計が必須である
  - 等変性（e.g., E(3)-equivariant）表現や、格子・座標・元素種を同時に扱う生成過程が鍵である

## 5. ユースケース
### 5.1 組織生成（GANが強い領域）
- 実験/シミュレーションの組織画像から、同等統計性を持つ合成組織を生成し、有限要素解析や物性評価の入力を大量に作る用途がある
- 条件付きGANにより、プロセス条件や組成条件を入力して組織の分布を生成する用途がある

### 5.2 結晶構造生成・結晶構造予測（拡散が伸びた領域）
- 結晶構造を、ノイズから格子・座標・元素種へ段階的に整形する生成として定式化し、安定結晶の分布を学習する研究が進んでいる
- 逆設計では、目標物性（バンドギャップ、形成エネルギー、磁性指標など）を条件として生成を誘導し、候補をDFTで検証する閉ループが基本である

### 5.3 ワークフロー（生成→フィルタ→緩和→検証）
生成モデルは、それだけで完結しないのが通常である。材料では次の後処理が現実的である。

1. 生成：候補を多数サンプルする
2. 妥当性チェック：化学量論、電荷中性、原子間距離閾値、PBC整合などで粗フィルタする
3. 緩和：力場・MLポテンシャル・DFTで構造緩和し、生成誤差を吸収する
4. 物性評価：高価な計算や実験を投入する候補を絞る
5. 反復：評価結果を条件モデルや誘導に反映し、探索を更新する

## 6. 評価指標
画像のFIDのような汎用指標だけでは不足しやすい。材料では次の観点が支配的である。

- Validity：物理・化学的に成立しているか（制約違反率）
- Uniqueness：同じものばかり生成していないか
- Novelty：学習データのコピーではないか（近傍探索、重複判定）
- Stability：緩和後に安定相へ落ちるか、形成エネルギーが妥当か
- Property satisfaction：条件付き生成で目標物性を満たす割合
- Statistics matching：組織なら相関関数、PSD、粒径分布などが一致するか

## まとめ
GANは高速生成と画像系の強さを持つ一方で、学習不安定性や多様性の欠損が課題になりやすい。拡散モデルは学習安定性と多様性で優位になりやすく、近年は結晶構造生成のような制約付き高次元生成にも適用が進んでいる。材料用途では、表現設計と物理制約、生成後の緩和・検証を前提にした閉ループ運用が実用上の鍵である。
